\documentclass{scrreprt}
\usepackage[x11names]{xcolor}	% colori
\usepackage{pgfplots}			% grafici
\usepackage{imakeidx}			% indice
\usepackage{amsfonts}			% font matematici	
\usepackage{amsthm}				% teoremi				
\usepackage{array}				% matrici
\usepackage{mdframed}			% cornici al testo
\usepackage{mathtools}			% matematica
\usepackage{hyperref}			% collegamenti ipertestuali per l'indice
\usepackage[italian]{babel}

\usepackage[a4paper,
top=2cm,
bottom=2cm,
includefoot,
left=2cm,
right=2cm,
footskip=1cm]{geometry}

\newcommand{\iseq}{\stackrel{?}{=}}

\newcommand{\SectionBreak}{%
	%\vskip 0.5ex

	\nointerlineskip
	\moveright 0.125\textwidth \vbox{\hrule width0.75\textwidth}
	\nointerlineskip
	%\vskip 0.5ex
	\makeatletter
		%\@afterindenfalse%
	\makeatother

}

% Impostazioni grafici pgfplots, altrimenti si lamenta
\pgfplotsset{compat=1.18}

% Definizione dell'ambiente "Definizione"
\newtheorem{defn}{Definizione}
\newenvironment{definition}{\begin{mdframed}[backgroundcolor=Ivory2]\begin{defn}}{\end{defn}\end{mdframed}}

% Definizione dell'ambiente "Teorema"
\newtheorem{teorema}{Teorema}
\newenvironment{thm}{\begin{mdframed}[backgroundcolor=Ivory2]\begin{teorema}}{\end{teorema}\end{mdframed}}

% Definizione dell'ambiente "Dimostrazione"
\newtheorem{demnstrn}{Dimostrazione}
\newenvironment{dimostrazione}{\begin{mdframed}[backgroundcolor=LightCyan1]\begin{demnstrn}}{\end{demnstrn}\end{mdframed}}

% Solo le equazioni referenziate vengono numerate
% vedi https://tex.stackexchange.com/a/2602
\mathtoolsset{showonlyrefs}  


\author{F. Piazza}
\title{%
	Appunti di Analisi Matematica II \\
	\large corso della prof.ssa B.Noris \\
	Politecnico di Milano\\
	\bigskip
	\bigskip

	\vspace{2cm}
	\begin{tikzpicture}
	\begin{axis}[axis line style={draw=none}, colormap/bluered]
	\pgfplotsset{ticks=none}
	% \addplot [domain=0:1, samples=100, color=black, thick] {x};
	 \addplot3[surf, samples=50, opacity=0.5, shader=interp] {x^2-y^2};
	\end{axis}
	\end{tikzpicture}
	\vspace{6cm}
}


\begin{document}
\maketitle

\section*{}

Gli esami non finiscono mai, amico mio! Così è la vita. \medskip \\
\null\hfill- \textit{Denis Ivanovič Fonvizin}
	


\tableofcontents

\setcounter{chapter}{-1} % per evitare che il capitolo 1 venga numerato come 2
\chapter{Cenni di Analisi I e Algebra Lineare}
\section{Regole di integrazione e derivazione}


\section{Serie numeriche}

\section{Determinante di una matrice}

\chapter{Equazioni differenziali}
\section{Equazioni differenziali del 1° ordine}
\begin{definition}
	Una equazione differenziale o \emph{EDO} del 1° ordine è una relazione tra una funzione $y$ e la sua derivata $y'$ che può essere scritta come
	\begin{equation}
		y' = f(y)
	\end{equation}
	dove $f$ è una funzione continua su un intervallo $I\subseteq\mathbb{R}$.
\end{definition}

Esempi:
\begin{itemize}
	\item \emph{Tema d'esame gennaio 2021}\\$y' = t \sqrt{y_{(t^2)}+1}$ è in forma normale con $f(t,s) = t \sqrt{s^2+1}$. Il dominio di $f$ è $I = \mathbb{R} \times \mathbb{R} = \mathbb{R}^2$.
	\item $y'_{(t)} = \frac{1}{t}$ con $t>0$ diventa $f(t,s) = \frac{1}{t}$. \textbf{Oss:} $f$ non dipende esplicitamente da $s$. \\ Il dominio di $f$ è $\{(t,s) \in \mathbb{R}^2 : s\in\mathbb{R}, t\in\mathbb{R}^* \}$, dunque è diviso in due parti.
			Dovrò quindi risolvere la EDO separatamente nelle due regioni.
			$$ \left\{\begin{array}{lr} y'(0)=\frac{1}{t},	t>0 \Rightarrow y(t) = ln(t)+c\\ y'(0)=\frac{1}{t},	t<0 \Rightarrow y(t) = ln(-t)+c \end{array}\right. $$
\end{itemize}
\begin{definition}
	Si chiama \emph{integrale generale} l'insieme delle soluzioni.
\end{definition}
\begin{definition}
	Si chiama \emph{soluzione particolare} una specifica soluzione.
\end{definition}
\noindent{Una EDO del 1° ordine ha $\infty^1$, soluzioni, cioè avrà una costante arbitraria. In modo analogo, una EDO del 2° ordine avrà $\infty^2$ soluzioni, cioè avrà due costanti arbitrarie.}
Esempi:
\begin{itemize}
	\item integrale generale $ce^t$ con $c$ costante arbitraria. Esempi di soluzioni particolari: $e^t$, $2e^t$, $-e^t$.
	\item $z_{(t)} = -1 + arctan(t)$ con $t\in\mathbb{R}^*$. Esempio di soluzione: $z' = 0 + \frac{1}{1+t^2}$.
\end{itemize}
\textbf{Oss:} La EDO $y'_{(t)} = f(t,y_{(t)})$ è definita per $(t,y)\in dom(f)$

\subsection{Soluzioni costanti di EDO del 1° ordine}
\begin{definition}
	Una soluzione costante di una EDO del 1° ordine è una funzione $y(t)$ che sia soluzione.
\end{definition}
Quando $y(t)=c$ è soluzione? Sostituisco $c$ a $y$:
\begin{equation}
	y'(t) = f(t,y(t)) \forall t
\end{equation}

Quindi \underline{le soluzioni costanti sono} $y(t) = c$ \underline{con $c$ tale che} $f(t,c) = 0 \forall t$.\\

Esempi:
\begin{itemize}
	\item Eq. Logistica: $y'(t) = ky(t)-hy^2(t)$\\
			$f(t,y)=ky-hy^2$\\
			$f(t,c)= 0 \forall t$ \quad $ky-hy^2=0=y(k-hy)$\\
			Soluzioni costanti: $y=0$ o $y=\frac{k}{h}$
	\item $y'(t)=te^{y(t)}$ \quad $te^{y(t)}=0$ non ha soluzione.
\end{itemize}

\SectionBreak

\subsection{EDO a variabili separabili}
\begin{definition}
	Una EDO del 1° ordine è detta \emph{a variabili separabili} se è del tipo
	\begin{equation}
		y' = f(t)\cdot g(y(t))
	\end{equation}
	dove $f$ e $g$ sono funzioni continue su intervalli $J_1,J_2\subseteq\mathbb{R}$.
\end{definition}

\begin{center}

	%scrivi in grassetto "Esempio"
	\textbf{Esempio}
	$y'(t)=\frac{1}{t}$ $\; h(t)=\frac{1}{t}$ $\;j_1=(-\infty,0)U(0,\infty)=\mathbb{R}$-{0}
\end{center}



\SectionBreak
\newpage
\subsection{Problema di Cauchy}
\begin{definition}
	Data una EDO del 1° ordine $y'_{(t)} = f(t,y_{(t)})$ sia $(t_{0}, y_{0})$ dove la EDO è definita.
	Cioè $(t_{0},y_{0})\in dom(f)$ \\
	Si chiama \emph{problema di Cauchy} il problema di determinare $y : I\subseteq \mathbb{R} \to \mathbb{R}$ che soddisfa:
	$$ \left\{\begin{array}{lr}y'(t)=f(t,y(t)) \\ y(t_{0})=y_{0}\end{array}\right. $$
\end{definition}
Nota: il sistema ha una condizione perché è del 1° ordine. La condizione trova la soluzione particolare che passa per $(t_{0},y_{0})$.\\
\subsection{Risolvere il problema di Cauchy}
Step:
\begin{enumerate}
	\item Trova l'integrale generale. ($\infty^1$ soluzioni dipendenti da 1 parametro)
	\item Impongo la condizione $y(t_{0})=y_{0}$ e la costante $c$
	\item Sostituisco $c$ in 1.
\end{enumerate}

\subsection{EDO 1° ordine lineari}
\begin{definition}
	Una EDO del 1° ordine lineare in forma normale è:\\
	\begin{equation}
		y'_{(t)} = a(t)y_{(t)} + b(t)
	\end{equation}
	dove $a$ e $b$ sono funzioni continue su un intervallo $J$ di $\mathbb{R}$.\\
	\textbf{N.B.} $J$ è il più grande intervallo di $\mathbb{R}$ tale che $a,b\in J$.\\
\end{definition}

\begin{definition}
	Si chiama EDO omogenea associata
	\begin{equation}
		y'_{(t)} = a(t)y_{(t)}\\
	\end{equation}
\end{definition}

% todo aggiungi esempi EDO lineari

\subsection{Principio di sovrapposizione}
Sia $a: J\subseteq\mathbb{R} \to \mathbb{R}$ una funzione continua su $J$.\\L'applicazione $\mathcal{L}(y)=y'-a(t)\cdot y$ è lineare.\medskip\\
Più esplicitamente, dati $c_1,c_2 \in \mathbb{R}$:\\
$\mathcal{L}(c_1y_1+c_2y_2)=c_1\mathcal{L}(y_1)+c_2\mathcal{L}(y_2) \forall y_1,y_2$ funzioni derivabili.\medskip\\
Ancora più esplicitamente:
se $\mathcal{L}(y_1)=b_1$ cioè $y'_1=a(t)y_1+b_1$\\
se $\mathcal{L}(y_2)=b_2$ cioè $y'_2=a(t)y_2+b_2$\\
allora $\mathcal{L}(c_1y_1+c_2y_2)=c_1b_1+c_2b_2$ cioè $y'_{(t)}=a(t)(c_1y_1+c_2y_2)+c_1b_1+c_2b_2$\\
cioè $(c_1y_1+c_2y_2)'=a(t)(c_1y_1+c_2y_2)+c_1b_1+c_2b_2$\\
\textbf{Oss:} \begin{itemize}
	\item Prendo due soluzioni distinde della EDO 
	\item  $y'=a(t)y + b(t)$\\
\end{itemize}
\SectionBreak
\subsection{Esistenza e unicità globale di Cauchy}
Siano $J\subseteq\mathbb{R}$ intervallo e $a,b: J\to \mathbb{R}$ continue.\\
Per ogni $t_0\in J, y_0 \in \mathbb{R}$ il problema di Cauchy:
\begin{equation}
	\left\{\begin{array}{lr}y'(t)=a(t)y(t)+b(t) \\ y(t_0)=y_0\end{array}\right.
\end{equation}
ha una soluzione unica $y: J\to \mathbb{R}$ definita su $J$.\\
\textbf{Aggiungi parte in blu lezione 16/09/2022}\\

\SectionBreak

\subsection{Teorema Formula risolutiva per EDO lineari 1° ordine}
$a,b: J\subseteq\mathbb{R}\to\mathbb{R}$ \quad $y'(t) = a(t)y(t)+b(t)$\\
L'integrale generale è dato dalla formula:
\begin{equation}
	y(t)=e^{A(t)}+\left( \int e^{-A(x)}b(x)dx + c \right) \quad \forall c\in \mathbb{R}
\end{equation}
dove $A(t)$ è una primitiva di $a$.\\

\begin{dimostrazione}
\emph{\textbf{da sapere all'esame}}\\
\begin{itemize}
	\item Porto $ay$ sulla sinistra\\
			$y'-ay=b$
	\item Moltiplico l'equazione per $e^{-A}$\\
			$e^{-A}y'-e^{-A}ay=e^{-A}b$
	\item Riconosco\\
			$y'(t)e^{-A(t)}-a(t)y(t)e^{-A(t)}=\left(y(t)e^{-A(t)}\right)$\\
			Quindi la EDO iniziale si riscrive equivalentemente:\\
			$(ye^{-A})'=be^{-A}$
	\item Integro\\
			$y(t)e^{-A(t)}=\int be^{-A(t)}dt+c$
	\item Moltiplico tutto per $e^{A(t)}$\\
			$y(t)=e^{A(t)}\left(\int be^{-A(t)}dt+c\right)$
\end{itemize}
\end{dimostrazione}

\SectionBreak

\subsection{Equazione di Bernoulli}
\begin{definition}
	Si chiamano \emph{equazione di Bernoulli} le EDO del 1° ordine lineari di forma:\\
	\begin{equation}
		y'_{(t)} = k(t)y_{(t)} + h(t)y_{(t)}^\alpha \quad \forall \alpha \in \mathbb{R}\setminus \{0,1\}
	\end{equation}
	con $k,y: J\subseteq\mathbb{R}\to\mathbb{R}$ continue.
\end{definition}

\textbf{Premesse:}\\
\begin{enumerate}
	\item Per semplificare ci occupiamo solo di soluzioni $y \geq 0$
	\item nel caso $\alpha<1$ accadono fenomeni strani, però la tecnica risolutiva è comunque valida
\end{enumerate}

\noindent{Procedimento di risoluzione:}\\
\begin{enumerate}
	\item Cerchiamo le soluzioni costanti (c'è sempre almeno quella nulla)
	\item divido per $y^\alpha$\\
			$y'(t) = k(t)y(t) + h(t)$\\
			$y'(t) = k(t)y(t)^{1-\alpha} + h(t)$
	\item Pongo $z(t)=y(t)^{1-\alpha}$\\
			Quale è l'equazione soddisfatta da $z$?\\
			$z'(t)= \left(1-\alpha\right)\left[k(t)y(t)^{1-\alpha}+h(t)\right] $\\
			$z'(t)=\left(1-\alpha\right)k(t)z(t)+\left(1-\alpha\right)h(t)$\\
	\item Risolvo l'equazione lineare in $z$
	\item Torno alla variabile $y = z(t)^{\frac{1}{1-\alpha}}$
\end{enumerate}

\SectionBreak

\subsection{Equazione Logistica}
$y(t) = $ numero di individui infetti al tempo $t$\\
$y: J\subseteq\mathbb{R}^+\to\mathbb{R}^+$
\bigskip\\
\textbf{1° Modello: Malthus (inizio '800)}\\
Il tasso di crescita della popolazione è proporzionale alla popolazione stessa.\\
\begin{equation}
	y'(t) = ky(t)
\end{equation}
dove $k\in\mathbb{R}^+$ è la \emph{tasso di crescita} e $k$ è il coefficiente di proporzionalità, dato dalla differenza tra tasso di natalità e tasso di mortalità.\\
integrale generale:
	$y(t) = y(0)e^{kt} $con$ c>0$\\


\noindent{\textbf{2° Modello: Verhulst (metà '800)}}\\
\begin{equation}
	y'(t)=ky(t)-hy(t)^2 \quad  \textnormal{con } k,h > 0
\end{equation}

Il modello prende anche in considerazione la competizione per le risorse al crescere della popolazione.\\
\bigskip
\emph{Simulazione numerica per $k=h=1$}

\begin{center}
\begin{tikzpicture}
	\begin{axis}[ymin = 0, ymax = 1.5, axis lines = left, height = 4.5cm, width = 8cm, legend pos = south east]

\addplot [
	domain=0:6, 
	samples=100, 
	color=red,
]
{1/(1-(1/3)*e^(-x))};
\addlegendentry{$y(0)=1.5$}
\addplot [
	domain=0:6, 
	samples=100, 
	color=blue,
	]
	{1/(1+e^(-x))};
\addlegendentry{$y(0)=0.5$}
\addplot [
	domain=0:6, 
	samples=100, 
	color=green,
	]
	{1};

	\end{axis}
\end{tikzpicture}	
\end{center}

\newpage % verifica
\textbf{Integrale generale dell'Equazione Logistica}\\
Trovo l'integrale generale risolvendo come Bernoulli
\begin{enumerate}

	\item Soluzioni costanti\hspace{1cm}  $y(t)=0,\hspace{0.5cm}   y(t)=\frac{k}{h}$
	\item Divido per $y^2: \hspace{0.5cm}   \frac{y'(t)}{y^2(t)} = \frac{k}{y(t)}-h$ 
	\item Pongo $z'(t)=\frac{1}{y(t)}=-\frac{k}{y(t)}+h=-kz(t)+h$ ricavo che $z'(t) + kz(t)=h$
	\item $z(t)= e^{-\int k}[\int e^{\int k}h dx+c]$\\\\$=e^{-kt}[h\int e^{kx} dx+c]$\\\\$=e^{-kt}[\frac{h}{k}e^{kt}+c]$\\\\$=\frac{\frac{h}{k}e^{kt}+c}{e^{kt}}$
	\item $y(t)=\frac{1}{z(t)}=\frac{e^{kt}}{\frac{h}{k}e^{kt}+c}=\frac{ke^{kt}}{he^{kt}+kc}$\\ possiamo scrivere $kc=c'$ in quanto costante arbitraria
	
\end{enumerate}

\newpage

\section[EDO 2° ordine lineari]{Equazioni differenziali ordinarie del 2° ordine lineari}{}
\subsection{Teorema di struttura dell'integrale generale di EDO del 2° ordine lineari omogenee}
Siano $a,b,c : I \subseteq \mathbb{R} \to \mathbb{R}$ funzioni continue e $a \neq 0$ in $I$.\\
L'integrale generale dell'eq. omogenea
\begin{equation}
	a(t)y''(t) + b(t)y'(t) + c(t)y(t) = 0
\end{equation}
è uno spazio vettoriale di dimensione 2, cioè le soluzioni sono tutte e sole della forma:
\begin{equation}
	y_0(t) = c_1y_{0_1}+c_2y_{0_2} \quad \textnormal{con } c_1,c_2 \in \mathbb{R}^n
\end{equation}
dove $y_{0_1},y_{0_2}$ sono due soluzioni linearmente indipendenti.\\


\emph{Oss:} Dire che due soluzioni sono linearmente indipendenti significa che non esiste un coefficiente $c$ tale che $c\cdot y_1 = y_2$, ovvero che non sono una multipla dell'altra.\\
\emph{Premesse:}
\begin{enumerate}
\item Spazio vettoriale $V=C^2(I)$
\item $I\subseteq\mathbb{R}$ funzione di 1 variabile $y(t)$
\item $C^2(I) = \{y:I \to \mathbb{R}$, derivabili in $I$ e $y'$ continua in $I\}$
\item $C^2(I) = \{y \in C^1(I)$, derivabili due volte in $I$ con $y''$ continua in $I\}$
\item $C^2(I)$ è uno spazio vettoriale con le operazioni usuali di somma di funzioni e prodotto di funzione per uno scalare.
\end{enumerate}

\begin{dimostrazione}
	

\emph{\textbf{da sapere all'esame}}
\begin{itemize}
\item L'integrale generale dell'omogenea è:\\
		$W = \{y \in V : ay''(t) + by'(t) + cy(t) = 0\}$
\item W è un sottospazio vettoriale di V $\Leftrightarrow$ è chiuso rispetto alla somma e rispetto al prodotto per uno scalare. Questo è vero grazie al principio di sovrapposizione (caso particolare dell'omogenea).
\item Devo dimostrare che W ha dimensione 2.
\begin{enumerate}
	\item[$i ) $] Determinare 2 soluzioni lineari indipendenti dell'equazione $y_{0_1}, y_{0_2}$
	\item[$ii )$] Dimostrare che ogni soluzione $y$ della EDO si scrive come combinazione lineare di $y_{0_1}, y_{0_2}$
	\item [$i )$] Scelgo $y_{0_1}$ soluzione del problema di Cauchy.\\
				$$
				\left\{
				\begin{array}{ll}
					ay''_{0_1}(t) + by'_{0_1}(t) + cy_{0_1}(t) = 0\\
					y_{0_1}(0) = 1\\
					y'_{0_1}(0) = 0
				\end{array} \right.
				$$
				Verifico che $y_{0_1}, y_{0_2}$ sono soluzioni lineari indipendenti. Se per assurdo fossero una multiplo dell'altra\\
				$ y_{0_1}(t) = \lambda y_{0_2}(t) \quad \forall t$\\
				In particolare, per $t=0$ avrei $y_{0_1}(0) = \lambda y_{0_2}(0)$ avrei trovato $1=\lambda\cdot 0$ assurdo.
	\item [$ii )$] Sia $y_0(t)$ soluzione dell'EDO, cerco $c_1,c_2\in \mathbb{R}$ tali che $y_0(t) = c_1y_{0_1}(t) + c_2y_{0_2}(t)$\\
					$ y_0(t) = c_1y_{0_1}(t) + c_2y_{0_2}(t) = c_1$\\
					\medskip
					$ y_0'(t) = c_1y_{0_1}'(t) + c_2y_{0_2}'(t) = c_2$\\
					In conclusione la funzione:\\
					$z(t) = y_0(0)\cdot y_{0_1}(t) + y_0'(0)\cdot y_{0_2}(t)$\\
					risolve lo stesso problema di Cauchy di $y_0(t)$ e quindi, grazie al teorema di esistenza e unicità di Cauchy, coincidono:\\
					$y_0(t) = z(t) \quad \forall t$,\\
					cioè $y_0(t)$ si scrive come combinazione lineare di $y_{0_1}, y_{0_2}$ con coefficienti $c_1=y_0(0)$ e $c_2=y_0'(0)$.\\
\end{enumerate}
\end{itemize}
\end{dimostrazione}
\SectionBreak

\subsection{Struttura dell'integrale generale di EDO del 2° ordine lineari non omogenee}
Siano $a,b,c:\mathbb{R} \to \mathbb{R}$ con $a\neq 0$ in $I$\\
L'integrale generale dell'eq. completa
\begin{equation}
	ay''(t) + by'(t) + cy(t) = f(t)
\end{equation}
è:
\begin{equation}
	y(t) = y_0(t) + y_p(t)
\end{equation}
dove la $y_0(t)$ è l'integrale dell'eq. omogenea, come nel teorema precedente, e la $y_p(t)$ è una soluzione particolare dell'eq. compleata.\\

\emph{Oss:} L'integrale generale di una EDO del secondo ordine lineare non omogenea è quindi uno spazio affine (cioè il translato di uno spazio vettoriale) di dimensione 2.\\
% ripasso gal

\begin{center}
	\textbf{Fine lezione 21/09 c'è una scritta in fondo in rosso che non so cosa sia.}
\end{center}



\newpage
\section{Sistemi differenziali lineari}
Esempio introduttivo $F=ma$
\begin{equation}
	y''(t) = \frac{F}{m}
\end{equation}
Trasformo in un sistema di due equazioni differenziali di primo ordine:\\
$ y_1(t) = y(t)$: posizione e $y_2(t) = y'(t) = y'_1(t)$: velocità.\\
$$ \left\{\begin{array}{lr} y'_1(t)=y_2(t) \quad \quad \textnormal{la velocità è la derivata della posizione}\\ y'_2(t)=\frac{F}{m} \quad \quad \textnormal{l'accelerazione è la derivata della velocità}\end{array}\right. $$
In forma matriciale:
\begin{equation}
	\left\{\begin{array}{c} y'_1(t) = y_2(t) \\ y'_2(t)= \frac{F}{m} \end{array}\right.
	\Leftrightarrow \left(\begin{array}{c} y'_1(t) \\ y'_2(t) \end{array}\right)
	= \left(\begin{array}{cc} 0 & 1 \\ 0 & 0 \end{array}\right)
	\cdot \left(\begin{array}{c} y_1(t) \\ y_2(t) \end{array}\right)
	+ \left(\begin{array}{c} 0 \\ \frac{F}{m} \end{array}\right)
\end{equation}



In forma compatta:
$\underline{y'}(t) = A\underline{y}(t)+\underline{b}(t)$\\
\begin{equation}
	\underline{y'}(t) = \begin{pmatrix} y_1(t) \\ y_2(t) \end{pmatrix} \quad A = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} \quad \underline{b} =\begin{pmatrix} 0 \\ \frac{F}{m} \end{pmatrix}
\end{equation}

% todo aggiungere esercizi


\begin{definition}
	\emph{Sistema differenziale lineare} è un sistema di equazioni differenziali di primo ordine con coefficienti costanti, cioè:
	\begin{equation}
		\underline{y'}(t) = A\underline{y}(t)+\underline{b}(t)
	\end{equation}
	dove $A$ è una matrice quadrata di ordine $n$ e $\underline{b}$ è un vettore di dimensione $n$.
\end{definition}
% ripasso gal

\subsection{Risultati teorici}
Come si scrive un problema di Cauchy per un sistema?\\
\emph{Esempio:} $y''(t)-2y'(t)=t$ con $y(0)=y_0$ e $y'(0)=v_0$\\
Trasformo la EDO in sistema
\begin{equation}
	\left\{
	\begin{array}{ll}
		y'_1(t) = y_2(t)\\
		y'_2(t) = 2y_2(t)+t
	\end{array}
	\right.
\end{equation}
Un problema di Cauchy è $$ \left\{ \begin{array}{ll} y_1(t_0) = y_0 \\ y_2(t_0)=v_0  \end{array} \right. \quad \quad \underline{y}(t_0) = \begin{pmatrix} y_0 \\ v_0 \end{pmatrix}$$\\

\begin{definition}
	Dato $\underline{y}'(t)=A\underline{y}(t)+\underline{b}(t)$ con $A$ matrice quadrata di ordine $n$ e $\underline{b}$ vettore di dimensione $n$\\
	chiamiamo problema di Cauchy: $$ \left\{ \begin{array}{ll} \underline{y}'(t) = A \cdot y(t) + \underline{b}(t) \\ \underline{y}(t_0) = \underline{y}_0  \end{array} \right.$$
\end{definition}

% todo esistenza e unicità problema di Cauchy

\section{Sistemi omogenei}
\begin{thm} Struttura dell'integrale generale di un sistema omogeneo\\
	Sia $A \in M_{n,n}(\mathbb{R})$. L'integrale generale del sistema differenziale omogeneo:
	\begin{equation}
		\underline{y}'(t) = A\underline{y}(t)
	\end{equation}
	è uno spazio vettoriale di dimensione $n$, cioè generato da $n$ soluzioni linearmente indipendenti:
	\begin{equation}
		\underline{y}_{0_1}(t), \cdots, \underline{y}_{0_n}(t)
	\end{equation}
	Cioè integrale generale: $\underline{y}(t) = c_1 \underline{y}_{0_1}(t) + \cdots + c_n \underline{y}_{0_n}(t)$ con $c_i \in \mathbb{R}$. 
	% todo che vuol dire cioè integrale generale?
\end{thm}

\begin{definition}
	Dato un sistema differenziale lineare $n\times n$ omogeneo, chiamo sistema fondamentale di soluzioni una famiglia di $n$ soluzioni linearimente indipendenti.
	$\underline{y}_{0_1}(t), \cdots, \underline{y}_{0_n}(t)$ sono una base dello spazio delle soluzioni.
\end{definition}

\subsection{Determinante Wronskiano}
Supponiamo di conoscere $n$ soluzioni $\underline{y}_{0_1}, \cdots, \underline{y}_{0_n}$ di un sistema differenziale lineare omogeneo $n\times n$.
\begin{thm}
	Sistema fondamentale $\Leftrightarrow$ esisete $t_0$ tale che $\det{\underline{y}_{0_1}(t_0) \cdots \underline{y}_{0_n}(t_0)} \neq 0$.
\end{thm}
\begin{definition}
	Si chiama matrice Wronskiana la matrice di funzioni ottenuta affiancando un sistema fondamentale di soluzioni:
	\begin{equation}
		W(t) = (\underline{y}_{0_1}(t), \cdots, \underline{y}_{0_n}(t))
	\end{equation}
\end{definition}
\emph{Oss:}
\begin{itemize}
	\item La matrice Wronskiana non è univocamente definita
	\item esiste $t_0$ tale che $\det{W(t_0)} \neq 0$
	\item Con la notazione $W(t)$ possiamo riscrivere in modo compatto l'integrale generale del sistema omogeneo $\underline{y}_0(t) = W(t) \cdot \underline{c}$
\end{itemize}

% todo ripasso diagonalizzabilità in R

\section{Sistemi non omogenei}
\subsection{Struttura dell'int. gen. dei sistemi non omogenei}
\begin{center}
	\emph{\textbf{Manca la definizione.}}
\end{center}

Praticamente, per risolvere un sistema non omogeneo:
\begin{enumerate}
	\item Si risolve il sistema omogeneo associato. Cioè determino una matrice Wronskiana $W(t) = \left[y_{0_1}(t)\ldots y_{0_n}(t)\right]$\\
			Integrale generale: $ \underline{y}(t) = W(t)\cdot \underline{c}, \quad \underline{c}\in\mathbb{R}^n$
	\item Due possibilità: \begin{itemize}
		\item cerco una soluzione particolare con il metodo di somiglianza \\ $\underline{y}(t) = \underline{y}_0(t) + \underline{y}_p(t) = W(t) \cdot \underline{c} + \underline{y}_p(t)$
		\item uso $W(t)$ per trovare direttamente l'integrale generale tramite la formula:\\$ \underline{y}(t) = W(t) \left(\int\left[W(\tau)\right]^{-1}\cdot \underline{b}(\tau)\right) = W(t) \int \left[W(\tau)\right]^{-1} \cdot \underline{b}(\tau) d\tau + W(t)\cdot \underline{c}$
		Confronto le due soluzioni.\\
		Nella prima ho $W(t)\cdot\underline{c}$ e anche nella seconda, ed è l'integrale generale dell'omogenea.\\
		Quindi $y_p(t) = \underline{y}(t) - W(t)\int \left[W(\tau)\right]^{-1} \cdot \underline{b}(\tau) d\tau$ perché è ciò che resta da uguagliare.\\
		In particolare, se $A$ è diagonalizzabile reale, allora una matrice Wronskiana è $W(t) = e^{At}$\\
		$\underline{y}(t) = e^{At} \left[\int e^{-At}\cdot \underline{b}(\tau) d\tau + \underline{c}\right] $\\
		\emph{Osservazioni da aggiungere.}\\
		\emph{Esempio da aggiungere.}	
	\end{itemize}



\end{enumerate}

\chapter{Serie di funzioni}


\section{Generalità sulle serie di funzioni}
Esempio: Sviluppi di Taylor:
\begin{equation}
e^x = \sum_{n=0}^\infty \frac{x^n}{n!} = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \ldots
\end{equation}
\begin{equation}
	\frac{1}{1-x} = \sum_{n=0}^\infty x^n = 1 + x + x^2 + x^3 + \ldots
\end{equation}

\begin{definition}
	\ \\
	Date delle funzioni $f: J\subseteq\mathbb{R}\to\mathbb{R}$ \quad $n=(0,1,2,\ldots)$
	\begin{itemize}
		\item La serie di funzioni a termine generale $f_n(x)$ è la successione delle somme parziali \\ $S_n(x) = \sum_{k=0}^n f_k(x)$\\
				\emph{Oss: }Fissato $\overline{x}\in J$ si tratta di una serie numerica
		\item La serie di funzioni converge puntualmente (o semplicemente) nel punto $\overline{x}_0\in J$ se la serie numerica di termini generale $f_n(\overline{x})$ è convergente, cioè se esiste finito il limite: \\ $\lim_{n\to\infty} S_n(\overline{x}) = \sum_{k=0}^\infty f_k(\overline{x})$
		\item Chiamiamo insieme di convergenza puntuale (o semplice) l'insieme $E \subseteq J$ dei punti $\overline{x}_0$ in cui la serie di funzioni converge puntualmente.\\ Nell'insieme $E$ risulta così definita una nuova funzione, detta somma della serie, che si indica con il simbolo: \\ $f(x) = \sum_{k=0}^\infty f_k(x) \quad (x \in E)$\\ Significa: \\ $f(x) = \lim_{n\to\infty} S_k(x) \quad (x \in E)$
		\item La serie di funzioni converge assolutamente in $\overline{x} \in J$ se la serie numerica di termine generale $f_n(\overline{x})$ converge.\\ \emph{Oss:} la convergenza assoluta...
	\end{itemize}
\end{definition}

\emph{Esempio fondamentale: Serie geometrica.}\\
$\sum_{n=0}^\infty x^n$\\
\emph{Oss.}Servirà per la serie di potenze: $\sum_{n=0}^\infty a_n(x-x_0)^n$\\
insieme convergenza puntuale $E = (-1,1)= \left\{ \left\lvert x\right\rvert <1\right\}$\\
Per $x \leq  -1$ è indeterminata\\
per $x \geq 1$ è divergente\\
per ... 
\bigskip\\
\textbf{Serie di Riemann}\\
$\sum_{n=0}^\infty \frac{1}{n^x} \quad \quad f_n(x)=\frac{1}{n^x}=\left(\frac{1}{n}\right)^x$
L'insieme di convergenza puntuale è:\\
$E=(1,\infty) = \left\{ x>1 \right\}$\\
La convergenza è anche assoluta in $E$ perché per $x\in E \quad \quad \left\lvert f_n(x)\right\rvert = f_n(x)$
\bigskip\\
Se $x=1$ è la serie armonica $\sum_{n=1}^\infty \frac{1}{n}$ che diverge.\\
\SectionBreak
\medskip
\emph{Problema:} se le funzioni $f_n$ sono continue, anche $f$ lo è? \textbf{Segue...}

\subsection{Convergenza totale di una serie di funzioni}
\emph{\textbf{copiare appunti della prof, formule e grafici}}\\
\begin{definition}
	Importante\\
	Diciamo che la serie di termine generale $f_n(x), x\in J$ converge totale in $I \in J$ se esiste una successione numerica $a_n$ tale che: 
	\begin{enumerate}
		\item[$i )$] $\lim_{n\to\infty} \left\lvert f_n(x) - a_n\right\rvert = 0 \quad \forall x \in I \quad \forall n$\\
		\item[$ii )$] $\sum_{n=0}^\infty a_n < \infty$
	\end{enumerate}
\end{definition}
\emph{Oss:} \begin{itemize}
	\item La nozione di convergenza totale riguarda un intervallo, mai un punto.
	\item La convergenta totale in $I$ implica la convergenza assoluta (quindi anche puntuale) in ogni punto di $I$.
	\item \textbf{Attenzione:} non vale il viceversa: se una serie converge assolutamente in ogni punto di $I$ non è detto che converga totalmente in $I$.
\end{itemize}




\section*{inizio lezione 05/10/2022}
\emph{Oss:} Se una serie di funzioni converge totalmente in $I$ allora converge totalmente in ogni sottoinsieme di $I$.\\
\textbf{Esercizio:} Studiare la convergenza totale della serie:\\
\begin{equation}
	\sum_{n=0}^\infty \frac{\sin{(nx)}}{n^3}
\end{equation}

\textbf{Esercizio:} Studiare la convergenza totale della serie geometrica:\\
\begin{equation}
	\sum_{n=0}^\infty x^n
\end{equation}

\textbf{\emph{Appunti su convergenza semplice, assoluta, puntuale, e totale}\\}
\subsection{Conseguenze della convergenza totale}
\begin{thm} Continuità della somma\\
	Siano $f_n(x)$ funzioni definite su un intervallo $I\subseteq \mathbb{R}$. Se:
	\begin{enumerate}
		\item[$i)$] $f_n(x)$ è continua in $I$ per ogni $n$
		\item[$ii)$] la serie di funzioni $\sum_{n=0}^\infty f_n(x)$ converge totalmente in $I$
	\end{enumerate}
	Allora la funzione somma $f(x) = \sum_{n=0}^\infty f_n(x)$ è continua in $I$.\\
\emph{Oss:} in particolare, $f$ è integrabile in ogni sottoinsieme chiuso e limitato $\left[c,d\right] \subseteq I$
\end{thm}

\begin{thm} Integrabilità termine a termine\\
	Nelle stesse ipotesi del teorema precedente $f$ è integrabile in ogni $\left[c,d\right] \subseteq I$ chiuso e limitato e inoltre:
	\begin{equation}
		\int_{c}^{d} f(x) dx = \int_{c}^{d} \left(\sum_{n=0}^\infty \int_{c}^{d} f_n(x)\right)  dx = \sum_{n=0}^\infty \left(\int_{c}^{d} f_n(x) dx\right) 
	\end{equation}
	Quindi posso scambiare il simbolo di serie e quello di integrale.
\end{thm}

\emph{Oss:} Se $f(n)$ derivabili in $I$ e $\sum^\infty f_n'$ converge totalmente in $I$ allora $\left(\sum^\infty f_n(x)\right)' = \sum^\infty f_n'(x)$





\section{serie di potenze}
\begin{definition}
	Una serie di potenze è una serie di funzioni della forma:
	\begin{equation}
		\sum_{n=0}^\infty a_n \left(x-x_0\right)^n = a_0+a_1(x-x_0)+a_2(x-x_0)^2+\dots 
	\end{equation}
	Con $a_n \in \mathbb{R}$ coefficienti della serie\\
	$x_0 \in \mathbb{R}$ centro della serie
\end{definition}
\emph{Convenzione:} se $x=x_0$ e $n=0$ $\left(x_0-x_0\right)^0 = 1$. Quindi, per $x=x_0$ la serie di potenze diventa:
\begin{equation}
	\sum_{n=0}^\infty a_n \left(x_0-x_0\right)^n = a_0+a_1(x_0-x_0)+a_2(x_0-x_0)^2+\dots = a_0
\end{equation}
Cioè tutte le serie di potenze convergono almeno nel loro centro $x=x_0$.\\

\section*{inizio lezione 07/10/2022}
\emph{ripasso convergenza totale}\\
\emph{integra}
\begin{itemize}
	

	\item vedremo in dettaglio che la serie esponenziale $e^x=\sum_{n=0}^\infty \frac{x^n}{n!} \quad a_n = \frac{1}{n!}$ ha come insieme di convergenza $\mathbb{R}$\\
			la serie $metti serie$ converge rapidamente. Criterio del rapporto $l=lim_{n\to\infty} \frac{a_{n+1}}{a_n} = \lim{n\to\infty}\frac{1}{n+1}=0<1$\\
	\item la serie logaritmica $ln(1+x)=\sum_{n=1}^\infty \frac{(-1)^{n+1}}{n}x^n \quad a_n = \frac{(-1)^{n+1}}{n}$ ha come insieme di convergenza $E=(-1,1)$\\
\end{itemize}

\begin{thm}
	Raggio di convergenza\\
	La serie di potenze reale $\sum_{n=0}^\infty a_n \left(x-x_0\right)^n$ si verifica sempre una delle tre:
	\begin{enumerate}
		\item raggio di convergenza nullo: la serie converge solo nel suo centro $x=x_0$
		\item raggio di convergenza infinito: la serie converge assolutamente  $\forall x\in\mathbb{R}$
		\item raggio di convergenza $0<R<+\infty$: esiste $R>0$ tale che:
																\begin{itemize}
																	\item la serie converge assolutamente $\forall x$ tale che $|x-x_0|<R$
																	\item la serie non converge per $|x-x_0|>R$
																\end{itemize}
	\end{enumerate}
	\emph{Oss:} in $|x_0+R|$ e $|x_0-R|$ potrebbe convergere o no, va studiato a parte in ogni esercizio.
\end{thm}

\begin{thm}
	Calcolo del raggio di convergenza\\
	Data una serie di potenze reale $\sum_{n=0}^\infty a_n \left(x-x_0\right)^n$\\
	\begin{enumerate}
		\item[$i)$] se il limite esiste.\\$R=\lim_{n\to\infty} \left\lvert \frac{a_n}{a_{n+1}}\right\rvert = \lim_{n\to\infty} \frac{|a_{n+1}|}{|a_n|}$\\allora la serie di potenze ha raggio di convergenza $R$.
		\item[$ii)$] se esiste il limite $R=\lim_{n\to\infty} \frac{1}{\sqrt[n]{\left\lvert a_n\right\rvert }}$ allora la serie di potenze ha raggio di convergenza $R$.
	\end{enumerate}
\end{thm}

\begin{dimostrazione} 
	\emph{\textbf{da sapere all'esame}}\\
	% \emph{teorema:} se $a_n \in \mathbb{R}$ e $a_n \to 0$ allora $\lim_{n\to\infty} \frac{a_n}{a_{n+1}} = 0$\\
	La serie di potenze converge assolutamente nel punto $\overline{x} \in \mathbb{R}$ se e solo se\\
	$\sum_{n=0}^\infty \left\lvert a_n\right\rvert  \left\lvert \overline{x} -x_0\right\rvert ^n$ converge.\\
	\begin{itemize}
		\item se il criterio del rapporto è applicabile, ho convergenze se e solo se\\$\lim_{n\to\infty} \frac{b_{n+1}}{b_n} < 1\Leftrightarrow \left\lvert \overline{x} -x_0 \right\rvert < \frac{1}{\lim_{n\to\infty} \frac{a_{n+1}}{a_n}} = \lim_{n\to\infty} \frac{\left\lvert a_n\right\rvert }{\left\lvert a_{n+1}\right\rvert } = R$
		\item se il criterio della radice è applicabile, la serie converge se e solo se\\$\lim_{n\to\infty} \sqrt[n]{b_n}<1$ \textbf{manca roba}
	\end{itemize}
\end{dimostrazione}

\begin{thm} % Convergenza totale per serie di potenze reali
	Data una serie di potenze avente raggio di convergenza $0<R\leq +\infty$ si ha:
	\begin{enumerate}
		\item[$i)$] se $R=+\infty$ la serie converge totalmente in ogni intervallo chiuso e limitato $\left[c,d\right] $
		\item[$ii)$] se $0<R<+\infty$ la serie converge totalmente in ogni intervallo chiuso  $\left[c,d\right] \subset \left(x_0-R,x_0+R\right)$
	\end{enumerate}
\end{thm}


\begin{thm}
	Data una serie di potenze reale $\sum_{n=0}^\infty a_n \left(x-x_0\right)^n$ avente raggio di convergenza $0<R<+\infty$, per ogni $x\in\left(x_0-R,x_0+R\right)$ vale la formula di integrazione termine a termine:
	\begin{equation}
		\int_{x_0}^{x} \sum_{n=0}^\infty a_n \left(t-x_0\right)^n dt = \sum_{n=0}^\infty \frac{a_n}{n+1} \left(x-x_0\right)^{n+1}
	\end{equation}
	La serie di potenze integrata $\sum_{n=0}^\infty \frac{a_n}{n+1} \left(x-x_0\right)^{n+1}$ converge per $x\in\left(x_0-R,x_0+R\right)$.
\end{thm}

\emph{Osservazione Importante:} Se la serie di potenze iniziale converge in $x_0-R$ (o $x_0+R$) posso integrare termine a termine fino a $x_0-R$ (o $x_0+R$).\\

\begin{center}
	$\cdots$
\end{center}


\begin{thm}
	Derivabilità termine a termine per serie di potenze reali\\
	Data una serie di potenze reale $\sum_{n=0}^\infty a_n \left(x-x_0\right)^n$ avente raggio di convergenza $0<R<+\infty$, per ogni $x\in\left(x_0-R,x_0+R\right)$ vale la formula di derivazione termine a termine.
	\begin{equation}
		\left(\sum_{n=0}^\infty a_n \left(x-x_0\right)^n\right)'  = \sum_{n=1}^\infty  a_n n \left(x-x_0\right)^{n-1}
	\end{equation}
	e la serie di potenze derivata ha raggio di convergenza $R$.\\
	Si può iterare per ottenere serie derivate di ogni ordine, tutte con raggio di convergenza $R$.

\end{thm}

\noindent\emph{Conseguenza:} la somma di una serie di potenze è derivabile ad ogni ordine.\\
\emph{Oss:} la serie derivata ha ancora raggio di convergenza $R$, ma il comportamente ai bordi $x_0\pm R$ può variare rispetto alla serie inerziale.\\

\section{Lezione del 12/10/2022}
\begin{definition}
	Una funzione di una variabile reale $f$ è detta analitica reale nell'intevallo non vuoto $(a,b)$ se è somma di una serie di potenze in $(a,b)$, cioè se esistono $x_0\in (a,b), a_n \in \mathbb{R}$ e tale che:\\
	$f(x) = \sum_{n=0}^\infty a_n \left(x-x_0\right)^n$ per ogni $x\in (a,b)$.
\end{definition}

Se $f$ è analitica in $(a,b)$:
\begin{itemize}
	\item Qual è la regolarità minima di $f$?\\
		\emph{Risposta:} $f$ derivabile ad ogni ordine in $(a,b)$.
	\item Chi sono i coefficienti $a_n$?\\
		\emph{Risposta:} $f(x_0) = \sum_{n=0}^\infty a_n \left(x-x_0\right)^n = a_0$\\
		$f'(x_0) = \sum_{n=1}^\infty a_n n \left(x-x_0\right)^{n-1} = a_1$\\
		$f''(x_0) = \sum_{n=2}^\infty a_n n(n-1) \left(x-x_0\right)^{n-2} = 2a_2$\\
		$a_n = \frac{f^{(n)}(x_0)}{n!}$
\end{itemize}

\begin{thm}
	\emph{Funzioni analitiche reali}\\
	Se $f$ è analitica reale nell'intervallo non vuoto $(a,b)$ allora è derivabile ad ogni ordine in $(a,b)$ e per ogni $x_0\in (a,b)$ è sviluppabile in serie di Taylor. Cioè:
	\begin{equation}
		f(x) = \sum_{n=0}^\infty \frac{f^{(n)}(x_0)}{n!} \left(x-x_0\right)^n \quad x\in (a,b)
	\end{equation}
	Inoltre, detto $R$ il raggio di convergenza della serie, l'identità è verificata per ogni $x\in (x_0-R,x_0+R)$.
\end{thm}

\subsection*{Serie esponenziale}
La funzione $e^x$ è analitica in $\mathbb{R}$ (non dimostriamo)\\
La sua serie di Taylor con $x_0=0$ è:
\begin{equation}
	e^x = \sum_{n=0}^\infty \frac{x^n}{n!} = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots
\end{equation}
Il raggio di convergenza è $R=+\infty$\\


\emph{oss:} Non tutte le funzioni derivabili ad ogni ordine in un intervallo sono analitiche in quell'intervallo. [es. BCFTV II.4]

\newpage
\section{serie di Fourier}
Segnale sonoro periodico:
\begin{equation}
	f(x)=3\cos(x)-\sin(2x)-\cos(10x)
\end{equation}

% grafico della funzione
\begin{center}
\begin{tikzpicture}
\begin{axis}
\addplot[domain=-10:10,samples=1000,smooth] {3*cos(deg(x)) - 2*sin(deg(2*x))-cos(deg(10*x))};
\end{axis}
\end{tikzpicture}
\end{center}

Per trasmettere il segnale basta comunicare i coefficienti:\\
%tabella dei coefficienti di sin e cos
\begin{center}
	\begin{tabular}{ c c c c c c c c}
	   & $n=0$ & $n=1$ & $n=2$ & $n=3$ & $\cdots$ & $n=10$ & $\cdots$	\\ 
	 coeff. di $\cos(nx$) & 0 & 3 & 0 & 0 & $\cdots$ & -1 & $\cdots$	\\  
	 coeff. di $\sin(nx$) & 0 & 0 & $-1$ & 0 & $\cdots$ & 0 & $\cdots$
	\end{tabular}
\end{center}

\emph{manca grafico}

Decomposizione di taylor non adatta perché:
\begin{enumerate}
	\item decomporre segnali non regolari
	\item individuare le frequenze alte
\end{enumerate}
La teoria di Taylor permette di decomporre un segnale periodico (non necessariamente regolare) nella combinazione lineare di infinite funzioni trigonometriche:
\begin{equation}
	f(x) \stackrel{?}{=} a_0 + \sum_{n=1}^\infty \left(a_n \cos(nx) + b_n \sin(nx)\right)
\end{equation}
\section*{Lezione del 14/10/2022}

\emph{Funzioni periodiche, polinomi e serie trigonometriche}\\
Ricordiamo che $f:\mathbb{R}\to\mathbb{R}$ è periodica di periodo $T$ se  $f(x)=f(x+T)$ per ogni $x\in\mathbb{R}$.\\
\emph{Oss:} 
\begin{itemize}
	\item Non ci sono ipotesi di regolarità su $f$.
	\item Se $f$ è periodica di periodo $T$ allora è anche periodica di periodo $2T, 3T, 4T \cdots$
	\item Se $f$ è periodica di periodo $T$ ed è pari (rispettivamente dispari) sul suo periodo, allora è anche pari (rispettivamente dispari) su $\mathbb{R}$.
\end{itemize}
% ripasso funzioni pari e dispari

\begin{definition}
	Chiamiamo armoniche $n$-esime le funzioni:
	\begin{equation}
		\cos(nx), \sin(nx)\quad \quad x\in\mathbb{R} n=1,2,3,\cdots
	\end{equation}
	Ogni armonica $n$-esima è periodica di periodo $\frac{2\pi}{n}$.
\end{definition}
\emph{Oss:} Tutte le armoniche $n$-esime sono anche periodiche di periodo $2\pi$\\
Caso speciale: $n=0$ la funzione costante 1.

\subsection{Formule di ortogonalità} % è una sottosezione? todo

% 0 se n!=k. pi se n=k!=0
\begin{equation}
	\int_{-\pi}^\pi \cos(nx)\cos(kx)dx =
		\begin{cases*}
			0 	& se  $n\neq k$\\
			\pi & se  $n=k\neq 0$
		\end{cases*}
\end{equation}
\begin{equation}
	\int_{-\pi}^\pi \sin(nx)\sin(kx)dx =
		\begin{cases*}
			0 	& se $n\neq k$\\
			\pi & se $ n=k\neq 0$
		\end{cases*}
\end{equation}
\begin{equation}
	\int_{-\pi}^\pi \cos(nx)\sin(kx)dx = 0
\end{equation}


\begin{definition}
	Un polinomio trigonometrico di ordine n è una combinazione lineare di  armoniche $n$-esime con $n=0,1,2,\cdots,m$, cioè:
	\begin{equation}
		a_0 + \sum_{n=1}^m \left(a_n \cos(nx) + b_n \sin(nx)\right)
		a_0, a_n, b_n \in \mathbb{R} \text{ e } x\in\mathbb{R}
	\end{equation}
\end{definition}
\emph{Oss:}
\begin{itemize}
	\item un polinomio trigonometrico è periodico di periodo $2\pi$
	\item la somma, differenza, prodotto di due polinomi trigonometrici è ancora un polinomio trigonometrico.
\end{itemize}

\begin{definition}
	Una serie trigonometrica è:
	\begin{equation}
		\sum_{n=0}^\infty \left(a_n \cos(nx) + b_n \sin(nx)\right)
	\end{equation}
	Con $a_0,a_n,b_n\in\mathbb{R}$
\end{definition}

Ogni polinomio trigonometrico è $2\pi$-periodico, quindi la somma di una serie trigonometrica è $2\pi$-periodica.\\
Per questo motivo supporremo sempre che la funzione che vogliamo decomporre sia $2\pi$-periodica.\\

% todo criterio di leibnitz ricordiamo

Derivando termine a termine una serie trigonometrica si può perdere regolarità. Questo differenzia le serie trigonometriche dalle serie di potenze, che sono sempre derivabili ad ogni ordine dentro il raggio di convergenza.

% todo integra la parte in blu appunti prof. probabilmente abbastanza superfluo

\begin{thm}
	Convergenza totale di una serie trigonometrica\\
	Data la serie trigonometrica $a_0 + \sum_{n=1}^\infty \left(a_n \cos(nx) + b_n \sin(nx)\right)$
	\begin{enumerate}
		\item[$i)$] Se $\sum_{n=1}^\infty \left(|a_n| + |b_n|\right) <+\infty$ allora la serie converge totalmente in $\mathbb{R}$.\\
				In particolare, la funzione somma è continua in $\mathbb{R}$ e posso integrare termine a termine in ogni sottoinsieme limitato.
		\item[$ii)$] Se $\sum_{n=1}^\infty n\cdot\left(|a_n| + |b_n|\right) =+\infty$ allora la funzione somma è derivabile in $\mathbb{R}$ e posso derivare termine a termine.
	\end{enumerate}
\end{thm}
\emph{Oss:} ci interesseranno anche le serie di Fourier che non convergono totalmente in $\mathbb{R}$, cioè tali che $\sum_{n=1}^\infty \left(|a_n| + |b_n|\right) =+\infty$
Infatti se il segnale periodico $f(x)$ che voglio comporre non è continuo su $\mathbb{R}$ allora la convergenza della serie trigonometrica non può essere totale in $\mathbb{R}$.

\subsection{Costruzione della serie di Fourier di una funzione periodica}
\begin{thm}
	Calcolo dei coefficienti di Fourier\\
	Sia $f:\mathbb{R}\to\mathbb{R}, 2\pi$ una funzione periodica e somma di una serie trigonometrica
	\begin{equation}
		f(x) = a_0 + \sum_{n=1}^\infty \left(a_n \cos(nx) + b_n \sin(nx)\right)
	\end{equation}
	Supponiamo inoltre di poter integrare termine a termine. Allora:
	\begin{equation}
		a_0 = \frac{1}{2\pi}\int_{-\pi}^\pi f(x)dx
	\end{equation}
	\begin{equation}
		a_n = \frac{1}{\pi}\int_{-\pi}^\pi f(x)\cos(nx)dx
	\end{equation}
	\begin{equation}
		b_n = \frac{1}{\pi}\int_{-\pi}^\pi f(x)\sin(nx)dx
	\end{equation}
\end{thm}

\begin{dimostrazione}
	\emph{\textbf{da sapere all'esame}}
	\begin{itemize}
		\item Integro $f$ in $\left(-\pi,\pi\right)$, uso integrazione temrine a termine e formula di ortogonalità:
			\begin{equation}
				\int_{-\pi}^\pi f(x)dx = \int_{-\pi}^\pi \left(a_0 + \sum_{n=1}^\infty \left(a_n \cos(nx) + b_n \sin(nx)\right)\right)dx
			\end{equation}
			\begin{equation}
				= \int_{-\pi}^\pi a_0dx + \sum_{n=1}^\infty a_n \int_{-\pi}^\pi \cos(nx)dx + \sum_{n=1}^\infty b_n \int_{-\pi}^\pi  \sin(nx)dx
			\end{equation}
			\begin{equation}
				a_0 = \int_{-\pi}^\pi 1dx = 2\pi a_0
			\end{equation}
		\item Per trovare $a_n$, moltiplico $f$ per $\cos{nx}$, integro in $\left(-\pi,\pi\right)$, uso l'integrabilità termine a termine  ele formule di ortogonalità:
			\begin{equation}
				\int_{-\pi}^\pi f(x)\cos(nx)dx = \int_{-\pi}^\pi \left(a_0 + \sum_{k=1}^\infty \left(a_k \cos(kx) + b_k \sin(kx)\right)\right)\cos(nx)dx
			\end{equation}
			\begin{equation}
				= a_0 \int_{-\pi}^\pi \cos(nx)dx + \sum_{k=1}^\infty a_k \int_{-\pi}^\pi \cos(kx)\cos(nx)dx + \sum_{k=1}^\infty b_k \int_{-\pi}^\pi \sin(kx)\cos(nx)dx
			\end{equation}
			\begin{equation}
				= a_0 \int_{-\pi}^\pi \cos^2(nx)dx = a_n\pi
			\end{equation}
		\item Per trovare $b_n$, moltiplico per $\sin{(nx)}$
	\end{itemize}
\end{dimostrazione}

Piano di lavoro: data $f:\mathbb{R}\to\mathbb{R}$, $2\pi$-periodica integrabile in $\left[-\pi,\pi\right]$:
\begin{enumerate}
	\item Calcolo i coefficienti $a_0, a_n, b_n$ con le formule trovate nel teorema precedente
	\item Con questi coefficienti costruisco la serie trigonometrica
	\item Studio la convergenza della serie trigonometrica e, in particolare cerco di stabilire se la somma coincide con $f$ in ogni punto
\end{enumerate}

\begin{definition}
	Sia $f:\mathbb{R}\to\mathbb{R}$, $2\pi$-periodica, integrabile in $\left[-\pi,\pi\right]$.
	\begin{itemize}
		\item Chiamiamo coefficenti di Fourier di $f$ i valori $a_0, a_n, b_n$ definiti nel teorema precedente
		\item polinomio di Fourier di $f$ di ordine $m$ il polinomio trigonometrico
			\begin{equation}
				F_m(x) = a_0 + \sum_{n=1}^m \left(a_n \cos(nx) + b_n \sin(nx)\right)
			\end{equation}
		\item Serie di Fourier di $f$ la serie trigonometrica
			\begin{equation}
				F(x) = \sum_{n=1}^\infty \left(a_n \cos(nx) + b_n \sin(nx)\right) = \lim_{m\to\infty} F_m(x)
			\end{equation}
	\end{itemize}
\end{definition}

\emph{\textbf{Importantissimo} per gli esercizi:}
\begin{itemize}
	\item se $f$ è pari, si sviluppa in soli coseni, cioè $b_n=0$ per ogni $n$.
	\item se $f$ è dispari, si sviluppa in soli seni, cioè $a_n=0$ per ogni $n$.
\end{itemize}

\section*{Lezione del 19/10/2022}

Esempio: dente di sega\\
$f(x) = x$\\
per $x \in (-\pi,\pi]$
Essendo $f$ dispari:
\begin{itemize}
	\item si sviluppa in soli seni, cioè $a_0=a_n=0 \quad \forall n$
	\item $b_n = \frac{1}{\pi} \int_{-\pi}^\pi f(x) \sin(nx)dx = \frac{2}{\pi} \int_{0}^\pi f(x) \sin{nx} = -2\frac{(-1)^n}{n}$
\end{itemize}
La serie di Fourier di $f$ è:
\begin{equation}
	-2\sum_{n=1}^\infty \frac{(-1)^n}{n} \sin(nx)
\end{equation}

\SectionBreak
\vspace*{0.5cm} % fix todo
Esempio: Tenda\\
$f:\mathbb{R}\to\mathbb{R}$, $2\pi$-periodica e\\
$f(x) = (x-\pi)^2$ per $x \in (0,2\pi]$\\
$f$ pari, quindi:
\begin{itemize}
	\item $b_n = 0 \quad \forall n$
	\item $a_n = \frac{2}{\pi} \int_{0}^\pi f(x) \cos{nx} dx = \frac{2}{\pi} \int_{0}^\pi (x-\pi)^2 \cos{nx} dx$
\end{itemize}
Quindi:
\begin{equation}
	a_0 = \frac{1}{2\pi} \int_{0}^\pi (x-\pi)^2 dx = \frac{\pi^2}{3} 
\end{equation}

\begin{equation}
	a_n = 
\end{equation}

\subsection{Convergenza della seride di Fourier}
Data $f:\mathbb{R}\to\mathbb{R}$, $2\pi$-periodica, integrabile in $\left[-\pi,\pi\right]$:
\begin{definition}
	Data $f:[-\pi,\pi]\to\mathbb{R}$. Diciamo che $f$ è regolare a tratti in $[-\pi,\pi]$ se esiste un numero finito di punti $x_0, x_1, \cdots, x_n$ in $[-\pi,\pi]$ tali che
	$f$ e derivabile in $(x_i,x_{i+1})\quad \forall i=0,1,\cdots,n-1$ ed esistono finiti i limiti:
	\begin{equation}
		\lim_{x\to x_i^-} f'(x) \quad \forall i=0,1,\cdots,n-1
	\end{equation}
	\begin{equation}
		\lim_{x\to x_i^+} f'(x) \quad \forall i=0,1,\cdots,n-1
	\end{equation}
\end{definition}

%todo disegno
%todo oggetti non verificabili (cuspide e asintoto verticale)

\emph{Oss:} Se $f$ è periodica e regolare a tratti in $[-\pi,\pi]$, allora:
\begin{itemize}
	\item $f$ è regolare a tratti in qualunque intervallo limitato.
	\item Si avrà anche che $f$ è continua in $(x_i,x_{i+1})\quad \forall i=0,1,\cdots,n-1$. ed esistono finiti i limiti:
	\begin{equation}
		\lim_{x\to x_i^-} f(x) \quad \forall i=0,1,\cdots,n-1
	\end{equation}
	\begin{equation}
		\lim_{x\to x_i^+} f(x) \quad \forall i=0,1,\cdots,n-1
	\end{equation}
	\item $f$ è integrabile su qualunque intervallo limitato.
	\item \textbf{Facoltativo:} BCFTV, esercizio $II.12$. La sua serie di Fourier è integrabile termine a termine.
\end{itemize}

\begin{thm} Della convergenza puntuale delle serie di Fourier\\
	Sia $f:\mathbb{R}\to\mathbb{R}$, $2\pi$-periodica, regolare a tratti in $[-\pi,\pi]$.\\
	Allora la serie di Fourier di $f$ converge puntualmente $\forall \mathbb{R}$ e inoltre
	\begin{equation}
		\lim_{m\to+\infty} F_m(x) = \frac{1}{2} \left[ \lim_{s\to x^+} f(s) + \lim_{s\to x^-} f(s) \right] 
	\end{equation}

Parafrasando: la serie di F converge puntualmente alla media tra $f(x^+)$ e $f(x^-)$.\\
In particolare:
\begin{equation}
	f \textnormal{continua in } x \Longrightarrow \lim_{m\to+\infty} F_m(x) = f(x)
\end{equation}
\end{thm}

\begin{thm} Convergenza totale della serie di Fourier\\
	Sia $f:\mathbb{R}\to\mathbb{R}$, $2\pi$-periodica, regolare a tratti in $[-\pi,\pi]$.\\
	Se inoltre $f$ è continua in tutto $\mathbb{R}$, allora la serie di Fourier di $f$ converge totalmente a $f$ in tutto $\mathbb{R}$.
\end{thm}

% verifica gli esempi di prima (tenda e dente di sega) ¿todo?


\section*{Lezione del 21/10/2022}
Ripasso\\
tde 16/02/2021

% funzione a tratti
\begin{equation}
	f(x) = \begin{cases}
		\pi-x & x \in (0,\pi] \\
		0 & x = 0
	\end{cases}
\end{equation}
f regolare a tratti, quindi al serie di Fourier converge $\forall x$\\
Funzione somma $S(x) = \lim_{x\to+\infty} F_m(x)$\\
Se $f$ è continua in $x$, allora $S(x) = f(x)$\\
Nei punti dove $f$ è discontinua, cioè $x=2k\pi$,\\
\begin{equation}
	S(x)= \frac{1}{2} \left( \lim_{s\to x^+} f(s) + \lim_{s\to x^-} f(s) \right) = 0 = f(x)
\end{equation}
Conclusione: $S(x) = f(x)$ $\forall x$\\
Riguardo la convergenza totale:\\
% todo aggiungi roba qua

In ogni intervallo dove $S$ è discontinua, la convergenza non può essere totale\\

\SectionBreak

\begin{thm} Convergenza in media quadratica\\
	Sia $f:\mathbb{R}\to\mathbb{R}$, $2\pi$-periodica, regolare a tratti in $[-\pi,\pi]$.\\
	Allora:
	\begin{equation}
		\lim_{m\to+\infty} \int_{-\pi}^\pi \left(F_m(x)-f(x)\right)^2 dx = 0 
	\end{equation}
\end{thm}
Spiegazione: si può dimostrare che la convergenza media quadratica implica:
\begin{equation}
	\lim_{m\to+\infty} \int_{-\pi}^\pi F_m(x)^2 dx = \int_{-\pi}^\pi f(x)^2 dx
\end{equation}
L'area sottesa al grafico di $F_m^2$ converge all'area sottesa al grafico di $f^2$ (per $m\to+\infty$)

\vspace{0.5cm}

\emph{Oss:}
\begin{itemize}
	\item Nuovo tipo di convergenza. È in realtà la convergenza più naturale. Infatti, si ha convergenza in media quadratica per qualunque funzione $f$ $2\pi$-periodica, tale che:
	\begin{equation}
	\int_{-\pi}^\pi f(x)^2 dx < +\infty
	\end{equation}
	\item $lim_{m\to+\infty} \int_{-\pi}^\pi \left(F_m(x)-f(x)\right)^2 dx = 0$ $\implies$  $lim_{m\to+\infty} \int_{c}^d \left(F_m(x)-f(x)\right)^2 dx = 0$\\
	per ogni intervallo limitato $[c,d]$
\end{itemize}

Calcoliamo $\int_{-\pi}^\pi F_m^2$:\\
\begin{equation}
	\int_{-\pi}^\pi F_m^2 dx = \pi \left(2a_0 + \sum_{n=1}^m a_n^2 + b_n^2\right)
\end{equation}

\begin{thm} Identità di Bessel-Parseval\\
	Sia $f:\mathbb{R}\to\mathbb{R}$, $2\pi$-periodica, regolare a tratti in $[-\pi,\pi]$.\\
	Allora:
	\begin{equation}
		\frac{1}{\pi}\int_{-\pi}^\pi f(x)^2 dx = 2a_0^2 \sum_{n=1}^\infty \left(a_n^2 + b_n^2\right)
	\end{equation}
\end{thm}
% todo ricorda convenzione per a_0

\subsection{Cenni: forma esponenziale - non in programma}
\begin{equation}
	\cos{x} = \frac{e^{ix} + e^{-ix}}{2} \quad \textnormal{e} \quad \sin{x} = \frac{e^{ix} - e^{-ix}}{2i}
\end{equation}
Sostituisco $x$ con $nx$ e ottengo:
\begin{equation}
	\cos{nx} = \frac{e^{inx} + e^{-inx}}{2} \quad \textnormal{e} \quad \sin{nx} = \frac{e^{inx} - e^{-inx}}{2i}
\end{equation}
\begin{equation}
	a_0 + \sum_{n=1}^\infty a_n \cos{nx} + b_n \sin{nx}
\end{equation}
\begin{equation}
	= a_0 + \sum_{n=1}^\infty \left( a_n \frac{e^{inx} + e^{-inx}}{2} + b_n \frac{e^{inx} - e^{-inx}}{2i} \right)
\end{equation}
\begin{equation}
	= a_0 + \sum_{n=1}^\infty \left( e^{inx} \left( \frac{a_n}{2} + \frac{b_n}{2i} \right) + e^{-inx} \left( \frac{a_n}{2} - \frac{b_n}{2i} \right) \right)
\end{equation}
Razionalizzo e ottengo:
\begin{equation}
	= a_0 + \sum_{n=1}^\infty \left( e^{inx} \frac{a_n-ib_n}{2} \right) + \sum_{n=1}^\infty \left( e^{-inx} \frac{a_n+ib_n}{2} \right)
\end{equation}
Nella seconda cambio indice $n=-k$ e ottengo:
\begin{equation}
	= a_0 + \sum_{n=1}^\infty \left( e^{inx} \frac{a_n-ib_n}{2} \right) + \sum_{k=1}^\infty \left( e^{ikx} \frac{a_k+ib_k}{2} \right)
\end{equation}
Rinomino i coefficienti in $c_0, c_n, c_k$ e ottengo:
\begin{equation}
	= c_0 + \sum_{n=1}^\infty \left( e^{inx} \frac{c_n}{2} \right) + \sum_{k=1}^\infty \left( e^{ikx} \frac{c_k}{2} \right)
\end{equation}
\begin{equation}
	= \sum_{n=-\infty}^\infty c_n e^{inx}
\end{equation}


\chapter{Spazio $\mathbb{R}^n$}
\section{Cenni di topologia in $\mathbb{R}^n$}

% todo richiami di gal + analisi vedi appunti prof

\begin{definition}
	Chiamiamo palla (o intorno sferico) in $\mathbb{R}^n$ di centro $\underline{x} _0 \in \mathbb{R}^n$ e raggio $r>0$ l'insieme:
	\begin{equation}
		B(\underline{x}_0) = \left\{\underline{x}\in\mathbb{R}^n \mid \left\|\underline{x}-\underline{x}_0\right\| < r\right\}
	\end{equation}
\end{definition}

\begin{definition}
	Dati un insieme $E\subset\mathbb{R}^n$ e un punto $\underline{x}_0\in\mathbb{R}^n$:
	\begin{itemize}
		\item $\underline{x}$ è di frontiera (o bordo) per $E$ se:
			\begin{equation}
				\forall r>0 \textnormal{ si ha che } \begin{cases}
					B_r(\underline{x}_0) \cap E \neq \emptyset \\
					B_r(\underline{x}_0) \cap E^c \neq 0
				\end{cases}
			\end{equation}
		\textbf{NB} i punti di frontiera possono appartenere all'insieme $E$ o no.
		\item $\underline{x}$ è interno a $E$ se: appartiene a $E$ e esiste $r>0$ tale che $B_r(\underline{x}_0) \subset E$.\\
		O, equivalentemente, se $\underline{x}$ appartiene a $E$ ma non è di frontiera.
		\item $\underline{x}$ è esterno a $E$ se: non appartiene a $E$ e esiste $r>0$ tale che $B_r(\underline{x}_0) \subset E^c$.\\
		O, equivalentemente, se $\underline{x}$ è interno a $E^c$.
	\end{itemize}
\end{definition}
Esempio:
\begin{equation}
	f(x,y) = \frac{\sqrt{xy}sin{xy} }{x^2+y^2}
\end{equation}
Dominio di definizione: $E = \left\{ (x,y) \in \mathbb{R}^2 \mid (x,y) \neq (0,0), xy \geq 0 \right\}$\\

% ripasso di nozioni di topologia in R todo

% se tolgo qualcosa a E cade la definizione??

\begin{definition}
	Un insieme $E\subset\mathbb{R}^n$ si dice:
	\begin{itemize}
		\item Aperto se $\forall \underline{x}_0 \in E \textnormal{ si ha che } \underline{x} \textnormal{ è un punto interno a } E$
		\item Chiuso se $E^c=\mathbb{R}^n\setminus E$ è aperto.
	\end{itemize}
\end{definition}
\emph{Oss:} Esistono insiemi né aperti né chiusi.\\
\emph{Oss:} $\mathbb{R}^n$ e $\emptyset$ sono aperti e chiusi.\\

\begin{definition}
	Un sottoinsieme di $\mathbb{R}^n$ è detto limitato esiste $R>0$ tale che $E\subset B_R(0)$. Altrimenti si dice illimitato. %perché R maiuscolo?
\end{definition}

% esempio todo

\section{Curve in nel piano e nello spazio}
\begin{definition}
	Una curva nello spazio è definita da:
	\begin{itemize}
		\item Tre funzioni \underline{continue}
				\begin{equation}
					t \in I \subseteq \mathbb{R} \longrightarrow  \left(\begin{array}{c} r_1(t)\\ r_2(t)\\ r_3(t) \end{array}\right)   = \underline{r}(t) \in \mathbb{R}^3
				\end{equation}
				è la parametrizzazione della curva.
		\item l'immagine dell'intervallo $I$ tramite $\underline{r}= ( r_1, r_2, r_3 )$, cioè:
				\begin{equation}
					y = \left\{ (x_1,x_2,x_3) \in \mathbb{R}^3 \mid x_1 = r_1(t), x_2 = r_2(t), x_3 = r_3(t) \textnormal{ per qualche } t \in I \right\}
				\end{equation}
				è il sostegno della curva
	\end{itemize}
\end{definition}
\emph{Oss:} Come caso particolare, quando $r_3(t)=0$ per ogni $t\in I$, si ha una curva nel piano.\\

% possibile esempio todo?

\emph{Oss:} il sostegno è univocamente determinato dalla parametrizzazione.\\
Il viceversa non è vero: esistono infinite parametrizzazioni associate al medesimo sostegno.\\

Def informale: due parametrizzazioni si dicono equivalenti se hanno il medesimo sostegno percorso lo stesso numero di volte (eventualmente in senso inverso).\\

\begin{definition}
	Due parametrizzazioni (continue) $\underline{r}(t): I \rightarrow \mathbb{R}^3$ e $\underline{s}(t): J \rightarrow \mathbb{R}$ si dicono equivalenti se esiste: $\phi (s): J \rightarrow I$ continua e biunivoca tale che:
	\begin{equation}
		\underline{v}(s) = \underline{r}(\phi(s)) = \underline{r} \circ \phi(s): J \rightarrow \mathbb{R}^3
	\end{equation}
\end{definition}

% esempio e osservazioni da valutare todo

\subsection{Curve regolari, versore tangente}
Introduzione informale:\\
Data una curva parametrica è molto facile calcolare in ogni punto la direzione tangente:
\begin{equation}
	\left( \begin{array}{c} x_1 \\ x_2 \end{array} \right) = \left( \begin{array}{c} r_1(t) \\ r_2(t) \end{array} \right) 
\end{equation}
per ogni $t$, la direzione tangente alla curva nel punto $\underline{r}(t)$ è:
\begin{equation}
	\left( \begin{array}{c} r'_1(t) \\ r'_2(t) \end{array} \right)
\end{equation}

\begin{definition}
	Una curva si dice regolare se ammette una parametrizzazione
	\begin{equation}
		\underline{r}(t) = \left( \begin{array}{c} r_1(t) \\ r_2(t) \end{array} \right)
	\end{equation}
	Tale che:
	\begin{enumerate}
		\item[$i)$] $r_1, r_2, r_3$ son funzioni di classe $C^1$ su $I$.
		\item[$ii)$] $\underline{r}'(t) = \left( \begin{array}{c} r'_1(t) \\ r'_2(t) \\ r'_3(t) \end{array} \right) \neq 0$ per ogni $t \in I$. (questo assicura che è sempre la tangente)
	\end{enumerate}
\end{definition}
% cosa vuol dire C^1? todo

\emph{Oss:} Richiediamo che il vettore non sia nullo, ma alcuen componenti possono essere 0, basta che non lo siano tutte.
% manca osservazione su curve regolari todo

% esempio di classe di curve regolarie todo

\begin{definition}
	Data una curva regolare, $\underline{r} : I \rightarrow \mathbb{R}^3$ definiamo $\forall t \in I$ il versore tangente:
	\begin{equation}
		\underline{T}(t) = \frac{\underline{r}'(t)}{\left\lVert \underline{r}'(t)\right\rVert }
	\end{equation}
	Il versore tangente ha:
	\begin{itemize}
		\item direzione parallela alla retta tangente ala curva nel punto $\underline{r}(t)$
		\item norma unitaria, cioè $\left\lVert \underline{T}(t)\right\rVert = 1$ $\forall t \in I$
		\item verso concorde con il verso di percorrenza della curva
	\end{itemize}
\end{definition}
Più esplicitamente:
\begin{equation}
	\underline{r}'(t) = \left( \begin{array}{c} r'_1(t) \\ r'_2(t) \end{array} \right) 
\end{equation}
\begin{equation}
	\left\lVert \underline{r}'(t) \right\rVert = \sqrt{r'_1(t)^2 + r'_2(t)^2}
\end{equation}
\begin{equation}
	\underline{T}(t) = \left(\begin{array}{c}
	\frac{r'_1(t)}{\left\lVert \underline{r}(t)\right\rVert }\\[2ex] % used to increase spacing
	\frac{r'_2(t)}{\left\lVert \underline{r}(t)\right\rVert }\\[2ex] % used to increase spacing
	\frac{r'_3(t)}{\left\lVert \underline{r}(t)\right\rVert }
	\end{array}\right)
\end{equation}

\subsection{Lunghezza di una curva}
\emph{Idea}: spezzettando la curva in archi progressivamente più piccoli, questi saranno quasi uguali al segmento relativo. Sommando le lunghezze dei segmenti si ottiene una stima della lunghezza della curva, che convergerà alla lunghezza vera e propria della curva quando la lunghezza dei segmenti tende a zero.\\

\begin{definition} \textbf{Lunghezza di una curva regolare}\\
	Data una curva regolare $[a,b] \subseteq \mathbb{R}$ e $\underline{r}: [a,b] \rightarrow \mathbb{R}^3$ la parametrizzazione di una curva regolare avente sostegno $\gamma$. Allora
	\begin{equation}
		\textnormal{lunghezza}(\gamma) = \int_a^b \left\lVert \underline{r}'(t) \right\rVert dt
	\end{equation}
\end{definition}

\begin{thm} Invarianza della lunghezza di una curva per riparametrizzazioni\\
	$[a,b] \subseteq \mathbb{R}$ e $\underline{r}: [a,b] \rightarrow \mathbb{R}^3$ la parametrizzazione di una curva regolare avente sostegno $\gamma$.\\
	$underline[v]: [c,d] \rightarrow \mathbb{R}^3$, $\underline{v}(s) = \underline{r}(\phi(s))$ è una parametrizzazione equivalente con sostegno $\delta$.\\
	Allora: lunghezza$(\gamma) =$ lunghezza$(\delta)$
\end{thm}

\begin{dimostrazione}.\\
	lunghezza$(\gamma) := \int_a^b \left\lVert \underline{r}'(t) \right\rVert dt$\\
	lunghezza$(\delta) := \int_c^d \left\lVert \underline{v}'(s) \right\rVert ds$\\
	\begin{equation}
		\underline{v}'(s) = \left\lVert \underline{r}'(\phi(s))\right\rVert  \cdot \left\lvert \phi'(s)\right\rvert 
	\end{equation}
	Quindi lunghezza$(\delta) = \int_c^d \left\lVert \underline{r}'(\phi(s))\right\rVert  \cdot \left\lvert \phi'(s)\right\rvert ds$\\
	Posso definizione di parametrizzazione eqauivalente, $\phi$ è biunivoca, cioè sempre crescente o sempre decrescente.\
	Supponiamo $\phi'(s) > 0$ per ogni $s \in [c,d]$.\\
	Allora lunghezza$(\delta) = \int_c^d \left\lVert \underline{r}'(\phi(s))\right\rVert  \cdot \phi'(s) ds$.\\
	Cambio di variabile nell'integrale $t=\phi(s)$ e $dt = \phi'(s) ds$\\
	\begin{equation}
		\int_a^b \left\lVert \underline{r}'(t) \right\rVert dt = \textnormal{lunghezza}(\gamma)
	\end{equation}
\end{dimostrazione}
\emph{Oss:} essendo $r_1(t), r_2(t), r_3(t) \in C^1([a,b])$ e $v_1(s), v_2(s), v_3(s) \in C^1([c,d])$ e $\underline{v} = \underline{r} \circ \phi$ allora $\phi: [c,d] \rightarrow [a,b]$ di classe $C^1$.
\begin{definition}
	Si dice regolare a tratti una curva $\underline{r}: I \rightarrow \mathbb{R}^3$ tale che:
	\begin{enumerate}
		\item[$i$)] $ \underline{r}$ continua in $I$
		\item[$ii$)] ad eccezione di un numero finito di valori $a < t_1 < t_2 < \cdots < t_n < b$ la curva è regolare.
	\end{enumerate}
\end{definition}

Lunghezza di una curva regolare a tratti:
\begin{equation} % todo verificare
	\textnormal{lunghezza}(\gamma) = \int_a^{t_1} \left\lVert \underline{r}'(t) \right\rVert dt + \int_{t_1}^{t_2} \left\lVert \underline{r}'(t) \right\rVert dt + \cdots \int_{t_n}^{b} \left\lVert \underline{r}'(t) \right\rVert dt
\end{equation}

\subsection{Integrale curvilineo}
Supponiamo che una curva regoalre (o regolare a tratti)
\begin{equation}
	\underline{r}: [a,b] \rightarrow \mathbb{R}^3
\end{equation}
Rappresenti un filo avente densità di massa
\begin{equation}
	\delta(t), \quad \textnormal{con} \quad \delta(t) \geq 0 \quad \forall t \in [a,b]
\end{equation}
massa del filo:
\begin{equation}
	M = \int_a^b \delta(t) \left\lVert \underline{r}'(t) \right\rVert dt
\end{equation}

\begin{definition}
	$[a,b] \subseteq \mathbb{R}$ e $\underline{r}: [a,b] \rightarrow \mathbb{R}^3$ la parametrizzazione di una curva regolare avente sostegno $\gamma$.\\
	$f(\underline{r}(t))$ continua, $t \in [a,b]$.\\
	L'integrale continuo di $f$ lungo $\gamma$, che si indica
	$ \int_{\gamma} f ds $ si calcola tramite la formula
	\begin{equation}
		\int_{a}^b f(\underline{r}(t)) \left\lVert \underline{r}'(t) \right\rVert dt
	\end{equation}
\end{definition}
\emph{Oss:}
\begin{itemize}
	\item il simbolo $\int_{\gamma} f ds$ è solo un simbolo % todo e quindi?
	\item Anche quando $f(\underline{r}(t)) \leq 0$ l'integrale curvilineo ha comunque senso (interpretazione diversa)
	\item Se $\gamma$ è regolare a tratti allora l'integrale curvilineo è definito come la somma degli integrali curvilinei dei tratti di $\gamma$.
\end{itemize}

\section*{Lezione 02/11/2022}

\section{Limiti e continuità per funzioni di 2 variabili}
\emph{Esempi introduttivi}
\begin{itemize}
	\item Probabilità: $X, Y$ variabili aleatorie, la funzione di dimostrazione congiunta $F(x,y)=P\left\{X\leq x, Y\leq y\right\} $
	\item Economia: la funzione di utilità descrive i panieri preferiti dal consumatore, ad esempio Cobb-Douglas $u(x,y)=x_1^{\alpha}x_2^{\beta}$
	\item Fisica: dato un corpo solido $A \subseteq \mathbb{R}^3$ la temperatura nel punto $(x,y,z) \in A$ al tempo $t \geq 0$ è  $u(x,y,z,t)$
	\item Fisica: n punti nel piano $\overline{P}_i = (x_i, y_i)$ di massa $m_i$ il momento di inerzia rispetto al generico punto $P=(x,y)$ è $I(x,y) = \sum_{i=1}^n m_i \left[(x-\overline{x}_i)^2+(y-\overline{y}_i)^2\right]$
\end{itemize}

\begin{definition}
	$A \subseteq \mathbb{R}^2$\\
	Una funzione di due variabili a valori reali $f: A \rightarrow \mathbb{R}$ è una relazione che associa ad ogni $(x,y) \in A$ un valore reale.
	\begin{equation}
		f: A \subseteq \mathbb{R}^2 \rightarrow \mathbb{R}
	\end{equation}
	\begin{equation}
		(x,y) \mapsto f(x,y)
	\end{equation}
	Chiamiamo insieme (o dominio) di definizione il più grande insieme su cui la $f$ è definita.\\

	\noindent Grafico($f$) = $\left\{(x,y,z) \in \mathbb{R}^3 \mid (x,y) \in A, z=f(x,y)\right\}$\\

	\noindent Per ogni $k \in \mathbb{R}$, l'insieme di livello di $f$ al livello $k$
	\begin{equation}
		I_k = \left\{(x,y) \in A \mid f(x,y)=k\right\}
	\end{equation}
\end{definition}

\subsection*{Integrale curvilineo di $f(x,y)$ lungo una curva piana} %todo va qua?
\begin{equation}
	\int_{\gamma} f(x,y) ds = \textnormal{area sottesa a $f(\underline{r}(t))$ con segno}
\end{equation}

\subsection{Limiti di funzioni di 2 variabili}
% ripasso limiti 1 variabile todo
% x-x0 < delta 
% 0<x-x0 < delta

\begin{definition}
	Siano $A \subseteq \mathbb{R}^2$ aperto, $\underline{x}_0 \in A$ e $f: A \setminus \left\{\underline{x}_0 \to \mathbb{R}\right\}$\\
	Diciamo che $f$ tende al limite $l \in \mathbb{R}$ per $\underline{x}$ che tende a $\underline{x}_0$ e scriviamo
	\begin{equation}
		\lim_{\underline{x} \to \underline{x}_0} f(\underline{x}) = l
	\end{equation}
	se per ogni $\varepsilon > 0$ esiste un $\delta > 0$ tale che se $\underline{x} \in B_{\delta}(\underline{x}_0) \setminus \left\{\underline{x}_0\right\} $ allora $|f(\underline{x})-l| < \varepsilon$.
\end{definition}

%todo rappresentazione grafica
%todo la prof ha parafrasato la definizione.

\emph{Oss:} \begin{itemize}
	\item $\underline{x}_0 \in B_{\delta}(\underline{x}_0) \setminus \left\{\underline{x}_0\right\} \Leftrightarrow \textnormal{distanza}(\underline{x}, \underline{x}_0) < \delta$
	\item si esclude il punto $\underline{x}_0$ nel quale $f$ potrebbe non essere definita
\end{itemize}

\subsubsection*{Non esistenza del limite}
% todo esempi
% esempio 2: lim xy -> 0 (x sqrt|y|) / (x^2 + 3y)
Conclusione: per stabilire che un limite 2D non esiste
\begin{equation}
	\lim_{\underline{x} \to \underline{x}_0} f(\underline{x})
\end{equation}
non esiste, è sufficiente esibire due cammini che arrivano in $\underline{x}_0$ tali che i limiti 1D di $f$ nei due cammini siano diversi.\\
Tali cammini possono essere rette o altre curve.

\subsubsection{Calcolo dei limiti con coordinate polari}
%todo esempio lim xy -> 0 (2x^2 y) / (x^2 + y^2)
\emph{Oss:} Può essere necessario uno step 0 se $f$ non è quozione ti polinomi:\\
% es 1: lim xy -> 0 (1-cos(xy)) / (x^2 + y^2)
Step 0: applico un limite notevole per ottenere una quozione di polinomi.\\


% \section*{Lezione del 9/11/2022}
\subsection{Continuità}
\begin{definition}
	$A \subseteq \mathbb{R}^2$ aperto, $\underline{x}_0 \in A$ e $f: A \to \mathbb{R}$\\
	Diciamo che $f$ è continua in $\underline{x}_0$ se
	\begin{equation}
		\lim_{\underline{x} \to \underline{x}_0} f(\underline{x}) = f(\underline{x}_0)
	\end{equation}
	$f$ è continua in $A$ se è continua in $\underline{x}_0$ per ogni $\underline{x}_0 \in A$.
\end{definition}

\emph{Nota:} Tutte le funzioni elementari in una variabile sono continue sul loro dominio di definizione.
Dovremo quindi verificare la continuità tramite la definizione solo per le funzioni definite per casi.

%todo esempi da valutare



% ----------------------------------------
\chapter{Calcolo differenziale per funzioni di più variabili}
\emph{Esempi:}
\begin{itemize}
	\item Funzione di utilità di Cobb-Douglas. Massimizzare $f(x,y) = \sqrt{xy}$ con il vincolo di budget $x+y=2$. 
	\item Staccionata di lunghezza $p$, recinto rettangolare. Massimizzare l'area recintata.
\end{itemize}

\section{Derivabilità e differenziabilità}
\subsection{Derivate parziali e gradiente}
\begin{definition}
	$A \subseteq \mathbb{R}^2$ aperto, $(x_0,y_0) \in A$ e $f: A \to \mathbb{R}$\\
	Le derivate parziali di $f$ in $(x_0,y_0)$ sono:
	\begin{equation}
		\frac{\partial f}{\partial x}(x_0,y_0) = \lim_{h \to 0} \frac{f(x_0+h,y_0)-f(x_0,y_0)}{h}
	\end{equation}
	\begin{equation}
		\frac{\partial f}{\partial y}(x_0,y_0) = \lim_{h \to 0} \frac{f(x_0,y_0+h)-f(x_0,y_0)}{h}
	\end{equation}
	Purché i limiti esistano finiti.

	Se esistono entrambe le derivate parziali di $f$ in $(x_0,y_0)$, diciamo che $f$ è derivabile in $(x_0,y_0)$.
	In tale caso, le derivare si organizzano in un vettore che prende il nome di gradiente.
	\begin{equation}
		\nabla f(x_0,y_0) = \left( \begin{array}[]{c} \frac{\partial f}{\partial x}(x_0,y_0) \\[2ex] \frac{\partial f}{\partial y}(x_0,y_0) \end{array} \right) \in \mathbb{R}^2
	\end{equation}
\end{definition}
% interpretazione geometrica todo
Sapendo che $f$ è derivabile nel punto $\underline{x}_0$, come calcolo $\nabla f(\underline{x}_0)$?\\
Applicando le usuali regole di derivazione a ciascuna variabile separatamente (considerando l'altra variabile come fosse costante)

%todo esempio

Ci sono solo due casi nei quali è necessario ricorrere alla definizione per stabilire se $f$ è derivabile e per il calolo delle derivate parziali:
\begin{enumerate}
	\item $f$ è definita per casi, cioè
	\begin{equation}
		f(x,y) = \begin{cases} g(x,y) & \textnormal{se } (x,y) \neq 0 \\ f(x,y) & \textnormal{in } \underline{x}_0 \end{cases}
	\end{equation}
	Calcolo $\frac{\partial f}{\partial x}(\underline{x}_0)$ e $\frac{\partial f}{\partial y}(\underline{x}_0)$ tramite la definizione.\\
	Se entrambi i limiti esistono finiti, $f$ è derivabile in $\underline{x}_0$ e ho ottenuto $\nabla f(\underline{x}_0)$.
	Altrimenti, $f$ non è derivabile in $\underline{x}_0$.
	\item nella definizione di $f$ compare $t^\alpha$ con $\alpha \in (0,1)$ oppure $\left\lvert t^{\alpha}\right\rvert $ con $\alpha \in (0,1]$ per qualche $t = g(x,y)$.\\
		Devo usare la definizione $\forall (x,y)$ tali che $g(x,y) = 0$.
\end{enumerate}
In tutti gli altri casi posso concludere che $f$ è derivabile senza ricorrere alla definizione.
% ripasso derivata analisi todo
% per funzioni non derivabili

% esempi todo ?
\emph{N.B.} $f$ derivabile in $\underline{x}_0$ non implica $f$ continua in $\underline{x}_0$.

\subsection{Differenziabilità e piano tangente}
\begin{definition}
	$A \subseteq \mathbb{R}^2$ aperto, $\underline{x}_0 \in A$ e $f: A \to \mathbb{R}$\\
	Diciamo che $f$ è differenziabile in $\underline{x}_0 = (x_0, y_0)$ se 
	\begin{enumerate}
		\item[$(i)$] $f$ è derivabile in $\underline{x}_0$
		\item[$(ii)$] $f(\underline{x}_0 + \underline{h}) = f(\underline{x}_0) + \langle \nabla f(\underline{x}_0), \underline{h} \rangle + R(\underline{h})$ con $R(\underline{h}) = o(\left\lVert \underline{h}\right\rVert )$ cioè $\lim_{\underline{h} \to (0,0)} \frac{R(\underline{h})}{\left\lVert \underline{h}\right\rVert } = 0$.
	\end{enumerate}
\end{definition}

\section*{Lezione 21/11/22}
% ripasso derivate parziali? todo

Esplicito: $\underline{x}_0 = (x_0, y0) \quad \quad \underline{h} = (h_1,h_2)$
\begin{equation}
	f(x_0 + h_1, y_0 + h_2) = f(x_0,y_0) + \frac{\partial f}{\partial x} (x_0, y_0) h_1 + \frac{\partial f}{\partial y} (x_0, y_0) h_2
\end{equation}
\begin{equation}
	\lim_{\underline{h} \to (0,0)} \frac{R(h)}{\left\lVert \underline{h} \right\rVert } = \lim_{(h_1, h_2) \to (0,0)} \frac{R(h_1, h_2)}{\sqrt[]{h_1^2 + h_2^2}} = 0
\end{equation}

Scrittura equivalente: ponendo $\underline{x} = \underline{x} + \underline{h}$
\begin{equation}
	f(\underline{x}) = f(\underline{x}_0) + \langle \nabla f(\underline{x_0}), \underline{x} - \underline{x}_0 \rangle + o(\left\lVert \underline{x} - \underline{x}_0 \right\rVert )
\end{equation}

\begin{definition}
	Se $f$ è defferenziabile in $\underline{x}_0 = (x_0,y_0)$, il piano tangente al grafico di $f$ in $(\underline{x}_0, f(\underline{x}_0))$ è
	\begin{equation}
		z = f(\underline{x}_0) + \langle \nabla f(\underline{x_0}), \underline{x} - \underline{x}_0 \rangle
	\end{equation}
\end{definition}
Esplicitamente:
\begin{equation}
	z = f(x_0,y_0) + \frac{\partial f}{\partial x} (x_0, y_0) (x - x_0) + \frac{\partial f}{\partial y} (x_0, y_0) (y - y_0)
\end{equation}
% esempio todo
% f = e^(2x-y) sapendeo che è differenziabile

\subsubsection{Come stabilire se $f$ è differenziabile in $\underline{x}_0$?}
\begin{definition}
	$A \subseteq \mathbb{R}^2$ aperto, $f: A \to \mathbb{R}$ derivabile.\\
	Se $\frac{\partial f}{\partial x}$ e $\frac{\partial f}{\partial y}$ sono continue in $A$, diciamo che $f$ è di classe $C^1$ in $A$ e scriviamo $f \in C^1(A)$.
\end{definition}
\begin{thm} \textbf{del differenziale totale}\\
	$A \subseteq \mathbb{R}^2$ aperto e $f \in C^1(A)$ allora $f$ è differenziabile in ogni punto di $A$.
\end{thm}
% ripete esempio di prima todo
Ci sono solo due casi in cui devo usare la definizione, gli stessi visti per la derivabilità.\\
La definizione richiede di verificare il limite di 2 variabili:
\begin{equation}
	\lim_{\underline{h} \to \underline{0}} \frac{R(\underline{h})}{\left\lVert \underline{h} \right\rVert } = \lim_{\underline{h} \to \underline{0}} \frac{f (\underline{x}_0 + \underline{h}) - f(\underline{x}_0) - \langle \nabla f(\underline{x}_0), \underline{h} \rangle}{\left\lVert \underline{h} \right\rVert } = 0
\end{equation}

\begin{thm} \textbf{Differenziabile implica continua}\\
	Siano $A \subseteq \mathbb{R}^2$ aperto, $\underline{x}_0 \in A$ e $f: A \to \mathbb{R}$ differenziabile in $\underline{x}_0$.\\
	Allora $f$ è continua in $\underline{x}_0$.
\end{thm}
\begin{dimostrazione}
	\emph{\textbf{da sapere all'esame}}\\
	Dobbiamo dimostrare che: $\lim_{\underline{x} \to \underline{x}_0} f(\underline{x}) = f(\underline{x}_0)$\\
	Essendo $f$ differenziabile in $\underline{x}_0$:\\
	\begin{equation}
		f(\underline{x}) - f(\underline{x}_0) = \langle \nabla f(\underline{x}_0), \underline{x} - \underline{x}_0 \rangle + o(\left\lVert \underline{x} - \underline{x}_0\right\rVert )
	\end{equation}
	\begin{equation}
		\left\lvert f(\underline{x}) - f(\underline{x}_0) \right\rvert = \left\lvert \langle \nabla f(\underline{x}_0), \underline{x} - \underline{x}_0 \rangle + o(\left\lVert \underline{x} - \underline{x}_0\right\rVert )\right\rvert 
	\end{equation}

	% todo importante passaggio oscuro
	\textbf{Manca il passaggio che non riesco a capire}\\

	Quindi:
	\begin{equation}
		\lim_{\underline{x} \to \underline{x}_0} \left\lvert f(\underline{x}) - f(\underline{x})\right\rvert = 0
	\end{equation}

	cioè
	\begin{equation}
		\lim_{\underline{x} \to \underline{x}_0} f(\underline{x}) = f(\underline{x}_0)
	\end{equation}
\end{dimostrazione}
% schema in blu di riassunto todo

\subsection{Altre proprietà delle funzioni differenziabili}
\subsubsection{Derivate direzionali e formula del gradiente}
\begin{definition}
	$A \subseteq \mathbb{R}^2$ aperto, $\underline{x}_0 = (x_0, y_0) \in A$ e $f: A \to \mathbb{R}$.\\
	$\underline{v} = (v_1, v_2)\in \mathbb{R}^2$ di norma unitaria, cioè $\left\lVert \underline{v} \right\rVert = 1$.\\
	La derivata direnzionale di $f$ in $\underline{x}_0$ nella direzione individuata da $\underline{v}$ è
	\begin{equation}
		\frac{\partial f}{\partial \underline{v}} (x_0, y_0) = \lim_{t \to 0} \frac{f(x_0 + t v_1, y_0 + t v_2) - f(x_0, y_0)}{t}
	\end{equation}
	Se il limite esiste finito.
\end{definition}
\emph{Oss:} le derivate parziali sono casi particolari di derivate direzionali.\\

\begin{thm} \textbf{Formula del gradiente}\\
	$A \subseteq \mathbb{R}^2$ aperto, $\underline{x}_0 \in A$ e $f: A \to \mathbb{R}$ differenziabile in $\underline{x}_0$.\\
	Allora $f$ ammette derivate direnzionali in ogni direzione $\underline{v}$ e inoltre
	\begin{equation}
		\frac{\partial f}{\partial \underline{v}} (\underline{x}_0) = \langle \nabla f(\underline{x}_0), \underline{v} \rangle
	\end{equation}
\end{thm}
\begin{dimostrazione}
	\emph{\textbf{da sapere all'esame}}\\
	Devo dimostrare che 
	\begin{equation}
		\lim_{t \to 0} \frac{f(\underline{x}_0 +t\underline{v})-f(\underline{x}_0)}{t} = \langle \nabla f(\underline{x}_0), \underline{v} \rangle
	\end{equation}
	Scelgo $\underline{h} = t \underline{v}$ nella definizione di differenziabilità:
	\begin{equation}
		f(\underline{x}_0 +t\underline{v})-f(\underline{x}_0) = \langle \nabla f(\underline{x}_0), t \underline{v} \rangle + o(\left\lVert t\underline{v}\right\rVert )
	\end{equation}
	Divido per $t$ e faccio il limite $t \to 0$:
	\begin{equation}
		\lim_{t \to 0} \frac{f(\underline{x}_0 +t\underline{v})-f(\underline{x}_0)}{t} = \langle \nabla f(\underline{x}_0), \underline{v} \rangle + 0
	\end{equation}
	% importante manca qualche calcolo, va bene comunque? todo
\end{dimostrazione}
% esempio di calcolo todo

\subsubsection{Derivata della composta}
1° CASO (più importante): derivata della restrizione di una funzione di 2 variabili a una curva piana. La restrizione di $f$ a $\underline{r}$ è la funizone composta $F(t) = (f \circ \underline{r})(t) = f(\underline{r}(t)) = f(r_1(t), r_2(t))$.\\
$F: J \subseteq \mathbb{R} \to \mathbb{R}$.\\
% finire lezione 11-11-22 minuto 1:37:00 todo

\section*{Lezione 16-11-22}
% ripasso differenziabilità
\subsubsection{Direzione di crescita massima, minima e nulla}
Se $f$ è differenziabile in $\underline{x}_0$:
\begin{equation}
	\frac{\partial f}{\partial \underline{v}} (\underline{x}_0) = \langle \nabla f(\underline{x}_0), \underline{v} \rangle \stackrel{?}{=} 0 % ci sono due opzioni negli appunti
\end{equation}
Cioè se $\nabla f(\underline{x}_0) \neq 0$ allora la derivata direzionale di $f$ in $\underline{x}_0$ è nulla nella direzione ortogonale a $\nabla f(\underline{x}_0)$.\\
% immagine 3d con spiegazione todo
\begin{thm} \textbf{ortogonalità del gradiente alle curve di livello} ovvero direzione di crescita nulla\\
	Sia $A \subseteq \mathbb{R}^2$ aperto, $\underline{x}_0 \in A$ e $f: A \to \mathbb{R}$ differenziabile in $A$. L'insieme di livello $I_k$ è il sostegno di una curva regolare $\underline{r}$.\\
	Allora:
	\begin{equation}
		\langle \nabla f(\underline{r}(t)), \underline{r}'(t) \rangle = 0 \quad \forall t
	\end{equation}
\end{thm}
Spiegazione: questo teorema dice 2 cose:
\begin{enumerate}
	\item[$i)$] se $\nabla f(\underline{x}_0) \neq 0$ allora $\nabla f(\underline{x}_0)\perp$ curva di livello passante per $\underline{x}_0$.
	\item[$ii)$] grazie alla formula del gradiente:
		\begin{equation}
			0 = \langle \nabla f(\underline{r}(t)), \underline{r}'(t) \rangle = \frac{\partial f}{\partial \underline{v}} (\underline{r}(t)) \quad  \textnormal{con } \underline{v} = \underline{r}'(t)
		\end{equation} 
		cioè la derivata direnzionale di $f$ nella direzione tangente alla curva di livello è nulla.
\end{enumerate}

\begin{dimostrazione}
	Per ipotesi $I_k$ coincide con il sostegno della curva regolare $\underline{r}(t)$, cioè:
	\begin{equation}
		I_k = \{ \underline{x}(t), t \in J \}
	\end{equation}
	In particolare $f(\underline{r}(t)) = k$ per ogni $t \in J$.\\
	Chiamo $F:J\to \mathbb{R}$ la funzione composta $F(t) = f(\underline{r}(t)) = f \circ \underline{r}(t)$.\\
	Da un lato $F(t) = k \quad \forall t \Rightarrow F'(t) = 0 \quad \forall t$.\\
	D'altro lato, per il teorema di derivazione della funzione composta
	\begin{equation}
		F'(t) = \langle \nabla f(\underline{r}(t)), \underline{r}'(t) \rangle \longrightarrow \langle \nabla f(\underline{r}(t)), \underline{r}'(t) \rangle = 0
	\end{equation}
\end{dimostrazione}

\begin{thm} \textbf{direzioni di massima e minima crescita}\\
	Sia $A \subseteq \mathbb{R}^2$ aperto, $\underline{x}_0 \in A$ e $f: A \to \mathbb{R}$ differenziabile in $A$. $\nabla f(\underline{x}_0) \neq 0$.\\
	Allora:
	\begin{enumerate}
		\item[$i)$] $\forall \underline{v} \in \mathbb{R}^2$ di norma unitaria si ha:
					\begin{equation}
						\left\lvert \frac{\partial f}{\partial \underline{v}} (\underline{x}_0) \right\rvert \leq \left\lVert  \langle \nabla f(\underline{x}_0), \underline{v} \rangle\right\rVert 
					\end{equation}
		\item[$ii)$] detti $\underline{v}_{\textnormal{max}} = \frac{\nabla f (\underline{x}_0)}{\left\lVert \nabla f (\underline{x}_0)\right\rVert }$ e $\underline{v}_{\textnormal{min}} = - \underline{v}_{\textnormal{max}}$ si ha:
					\begin{equation}
						\frac{\partial f}{\partial \underline{v}_{\textnormal{max}} } (\underline{x}_0) = \left\lVert \nabla f (\underline{x}_0) \right\rVert \quad \textnormal{e} \quad \frac{\partial f}{\partial \underline{v}_{\textnormal{min}} } (\underline{x}_0) = - \left\lVert \nabla f (\underline{x}_0) \right\rVert
					\end{equation}
	\end{enumerate}
\end{thm}
% idea di dimostrazione todo

\section{Ottimizzazione libera}
Derivate seconde e matrice Hessiana.

\begin{definition}
	$A \subseteq \mathbb{R}^n$ aperto, $f: A \to \mathbb{R}$ derivabile.\\
	Se le derivate parziali $\frac{\partial f}{\partial x} (x,y)$ e $\frac{\partial f}{\partial y} (x,y)$ sono a loro volta derivabili in $A$, definiamo le derivate seconde:
	\begin{equation}
		\frac{\partial^2 f}{\partial x^2} (x,y) = \frac{\partial}{\partial x} \left( \frac{\partial f}{\partial x} (x,y) \right) \quad \quad \quad \frac{\partial^2 f}{\partial y \partial x} (x,y) = \frac{\partial}{\partial y} \left( \frac{\partial f}{\partial x} (x,y) \right)
	\end{equation}
	\begin{equation}
		\frac{\partial^2 f}{\partial x \partial y} (x,y) = \frac{\partial}{\partial x} \left( \frac{\partial f}{\partial y} (x,y) \right) \quad \quad \quad \frac{\partial^2 f}{\partial y^2} (x,y) = \frac{\partial}{\partial y} \left( \frac{\partial f}{\partial y} (x,y) \right)
	\end{equation}
	Si organizzano in una matrice detta Hessiana:
	\begin{equation}
		H_f (x_0, y_0) = \begin{pmatrix}
			\frac{\partial^2 f}{\partial x^2} (x_0, y_0) & \frac{\partial^2 f}{\partial y \partial x} (x_0, y_0) \\
			\frac{\partial^2 f}{\partial x \partial y} (x_0, y_0) & \frac{\partial^2 f}{\partial y^2} (x_0, y_0)
		\end{pmatrix}
	\end{equation}
\end{definition}
% esempio todo

\begin{definition}
	Se $f$ è derivabile due volte in $A$ e tutte le derivate parziali seconde sono continue in $A$, diciamo che $f$ è di classe $C^2$ in $A$: $f \in C^2(A)$.
\end{definition}
Dovrei verificare "a mano" se $f$ sia di classe $C^2$ o meno nei casi seguenti:
\begin{itemize}
	\item $f$ definita per casi
	\item compare $t^\alpha$ con $\alpha \in (0,2)$ o $\left\lvert t^\alpha \right\rvert$ con $\alpha \in (0,2]$
\end{itemize}
In tutti gli altri casi $f$ è automaticamente di classe $C^2$ sul suo dominio di definizione, quindi non serve verificare.

\begin{thm} \textbf{di Schwarz}
	$A \subseteq \mathbb{R}^n$ aperto, $f: A \to \mathbb{R}$ di classe $C^2$ allora:
	\begin{equation}
		\frac{\partial^2 f}{\partial y \partial x} (x_0, y_0) = \frac{\partial^2 f}{\partial x \partial y} (x, y) \quad \forall (x,y) \in A
	\end{equation}
	Cioè $H_f(x,y)$ è una matrice simmetrica $\forall (x,y) \in A$.
\end{thm}
Posso quindi associare una forma quadratica e calcolarne il segno.

\begin{definition}
	Sia $f \in C^2(A)$. Chiamiamo forma quadratica indotta da $H_f(x,y)$:
	\begin{equation}
		q: \mathbb{R}^2 \to \mathbb{R} \quad \quad q(h_1,h_2) = (h_1, h_2) \cdot H_f(x_0,y_0) \binom{h_1}{h_2} = \langle \binom{h_1}{h_2}, H_f(x_0,y_0) \cdot \binom{h_1}{h_2} \rangle % non binomaile ma vettore todo 
	\end{equation}
% esplicito q todo importante
%
%
\end{definition}
%ripasso gal forme quadratiche todo

% esempio

\subsubsection{Formula di Taylor al 2° ordine}
% richiamo analisi 1 todo
\begin{thm} \textbf{Formula di Taylor al 2° ordine}\\ % è un teorema? todo importante
	$A \subseteq \mathbb{R}^n$ aperto, $f \in C^2(A)$  allora per ogni $x_0 \in A$:
	\begin{equation}
		f(\underline{x}_0 + \underline{h}) = f(\underline{x}_0) + \left\langle \nabla f (\underline{x}_0), \underline{h}\right\rangle  + \frac{1}{2} \left\langle \underline{h}, H_f (\underline{x}_0) \underline{h}\right\rangle + o(\left\lVert \underline{h} \right\rVert^2)
	\end{equation}
\end{thm}
% osservazioni in blu todo
Ponendo $\underline{x}_0 + h = \underline{x}$:
\begin{equation}
	f(\underline{x}) = f(\underline{x}_0) + \left\langle \nabla f (\underline{x}_0), \underline{x} - \underline{x}_0\right\rangle  + \frac{1}{2} \left\langle \underline{x} - \underline{x}_0, H_f (\underline{x}_0) (\underline{x} - \underline{x}_0)\right\rangle + o(\left\lVert \underline{x} - \underline{x}_0 \right\rVert^2)
\end{equation}

% todo richiami di ottimizzazione analisi 1

\subsection{Introduzione all'ottimizzazione 3D}
\begin{definition}
	Sia $A \subseteq \mathbb{R}^2$ sottoinsieme qualunque, $f: A \to \mathbb{R}$.\\
	Un punto $(x_0,y_0) \in A$ si dice:
	\begin{itemize}
		\item punto di massimo globale (o assoluto) per $f$ in $A$ se $f(x_0,y_0) \geq f(x,y)$ per ogni $(x,y) \in A$. Il valore di $f(x_0,y_0)$ è detto (valore di) massimo globale o assoluto.
		\item Punto di massimo locale (o relativo) per $f$ in $A$ se $f(x_0,y_0) \geq f(x,y)$ per ogni $(x,y) \in B_\delta (x_0,y_0) \cap A$. Il valore di $f(x_0,y_0)$ è detto (valore di) massimo locale o relativo.
		\end{itemize}
	Analogamente minimo invertendo le disuguaglianze.
\end{definition}
\emph{Oss:}\begin{itemize}
	\item I punti di massimo appartengono all'insieme $A$ e possono essere interni o di frontiera.
	\item Il valore di max assoluto è unico, ma potrebbe essere assunto in più punti.
\end{itemize}

\begin{definition}
	Se $(x_0,y_0)$ è un punto di max o di min per $f$ in $A$ allora è detto punto di estremo per $f$ in $A$.
\end{definition}

\begin{thm} \textbf{di Weierstrass}
	$A \subseteq \mathbb{R}^2$ chiuso e limitato, $f: A \to \mathbb{R}$ continua.\\
	Allora $f$ assume i valori di massimo e minimo assoluto, cioè esistono:
	\begin{itemize}
		\item $(x_m, y_m) \in A$ punto di minimo assoluto.
		\item $(x_M, y_M) \in A$ punto di massimo assoluto.
	\end{itemize}
	Cioè:
	\begin{equation}
		f(x_m, y_m) \leq f(x,y) \leq f(x_M, y_M) \quad \forall (x,y) \in A
	\end{equation}
\end{thm}


% anticipazione esempio todo
\subsection{Ottimizzazione libera}
Cioè su un insieme aperto si applica Fermat, ma non Weierstrass.
\begin{thm}
	Sia $A \subseteq \mathbb{R}^2$ aperto, $f: A \to \mathbb{R}$.\\
	Se $(x_0,y_0)$ è un punto di estremo per $f$ e se $f$ e se $f$ è derivabile in $(x_0,y_0)$ allora:
	\begin{equation}
		\nabla f(x_0,y_0) = \underline{0}
	\end{equation}
\end{thm}
\begin{definition}
	Sia $A \subseteq \mathbb{R}^2$ aperto, $f: A \to \mathbb{R}$ derivabile in $(x_0,y_0) \in A$.\\
	Se $\nabla f(x_0,y_0) = \underline{0}$ allora $(x_0,y_0)$ è detto punto critico o punto stazionario per $f$.
\end{definition}
Quindi secondo il teorema di Fermat se un punto è estremale allora è anche critico.

\begin{definition}
	Un punto critico di $f$ che non è estremale per $f$ è detto punto di sella.
\end{definition}
In pratica i punti estremali di una funzione $f$ in un insieme $A$ aperta:
\begin{enumerate}
	\item se $f$ è derivabile in tutto $A$: determino i punti critici di $f$ in $A$ e poi stabilisco quali sono di massimo, minimo, sella.
	\item Se $f$ ha punti di non derivabilità li includo tra i candidati.
\end{enumerate}
\emph{Oss:} Se $f$ è anche differenziabile in $(x_0, y_0)$ punto critico allora:
\begin{enumerate}
	\item il piano tangente al grafico di $f$ in $(x_0, y_0, f(x_0, y_0))$ è orizzontale:
			\begin{equation}
				z = f(x_0, y_0)
			\end{equation}
	\item $\forall \underline{v} \in \mathbb{R}^2$, $\left\lVert \underline{v} \right\rVert = 1$, si ha:
			\begin{equation}
				\frac{\partial f}{\partial \underline{v}}(x_0, y_0) = 0
			\end{equation}
\end{enumerate}
%esempio
\subsection{Classificazione dei punti critici}
\begin{thm} \textbf{Criterio della matrice Hessiana}\\
	$A \subseteq \mathbb{R}^2$ aperto, $f \in C^2(A)$.\\
	$\underline{x}_0 = (x_0, y_0) \in A$ punto critico di $f$ allora.\\
	Denotiamo $q$ la fomra quadratica indotta da $H_f(\underline{x}_0)$, cioè:
	\begin{equation}
		q(h_1,h_2) = (h_1, h_2) \cdot H_f(\underline{x}_0) \cdot \begin{pmatrix} h_1 \\ h_2 \end{pmatrix}
	\end{equation}
	Allora:
	\begin{enumerate}
		\item[$i)$] Se $q$ è definita positiva allora $\underline{x}_0$ è punto di minimo.
		\item[$ii)$] Se $q$ è definita negativa allora $\underline{x}_0$ è punto di massimo.
		\item[$iii)$] Se $q$ è indefinita allora $\underline{x}_0$ è punto di sella.  
	\end{enumerate}
\end{thm}
\emph{Oss:} Se $q$ è indefinita il criterio della matrice Hessiana non da informazioni.
\begin{dimostrazione}
	Essendo $\nabla f(\underline{x}_0) = \underline{0}$, la formula di Taylor al secondo ordine diventa
	\begin{equation}
		f(\underline{x}_0 + \underline{h}) = f(\underline{x}_0) + \frac{1}{2} q(\underline{h}) + o(\left\lVert \underline{h} \right\rVert^2)
	\end{equation}
	\begin{enumerate}
		\item[$i)$] Se $q$ è definita positiva, cioè $q(\underline{h})>0 \quad \forall \underline{h}$\\
				
	\end{enumerate}
	% todo importante dimostrazione
	\begin{center}
		Da finire
	\end{center}

\end{dimostrazione}
\subsubsection{Come applicare il criterio della matrice Hessiana}
\begin{enumerate}
	\item[$i)$] $\det H_f(\underline{x}_0) > 0$ e $\frac{\partial^2 f}{\partial x^2}(x_0, y_0) > 0$ allora minimo.
	\item[$ii)$] $\det H_f(\underline{x}_0) > 0$ e $\frac{\partial^2 f}{\partial x^2}(x_0, y_0) < 0$ allora massimo.
	\item[$iii)$] $\det H_f(\underline{x}_0) < 0$ allora punto di sella.
	\item[$iv)$] $\det H_f(\underline{x}_0) = 0$ allora il criterio non si applica.
\end{enumerate}

\section*{Lezione 23-11-22}
Cosa fare quando $\det H_f(\underline{x}_0) = 0$ con $\nabla f(\underline{x}_0) = \underline{0}$ e $\underline{x}_0 \in A$ aperto?\\
Ciop il criterio dell'Hessiana non da informaizoni.\\

INDAGINE DIRETTA (O STUDIO LOCALE)\\
% esempio todo punti di estremo di f(x,y) = x^4 - 6x^2y^2 + y^4
Altra tecnica: studio del segno di $f$ attorno al punto candidato.\\

CONVESSITÀ E CONCAVITÀ\\
\begin{definition}
	$f: \mathbb{R}^2 \to \mathbb{R}$, $f \in C^2(A)$ è convessa in $\mathbb{R}^2$ se $\forall (x,y) \in \mathbb{R}^2$ $H_f(x,y)$ è definita positiva o semidefinita positiva.\\
\end{definition}
\emph{Oss:} Simile alla dimenzione 1.\\

Ottimizazione per funzioni convesse\\ %todo importante definizione o teorema?
Sia $f$ $C^2(A)$ convessa.
Se $\underline{x}_0 \in \mathbb{R}^2$ è un punto critico di $f$ allora $\underline{x}_0$ è punto di minimo assoluto.

\begin{definition}
	$f \in C^2(A)$ è concava se $\forall (x,y) \in \mathbb{R}^2$ si ha $H_f(x,y)$ è definita negativa o semidefinita negativa.\\
	In questo caso $\underline{x}_0$ è punto di massimo assoluto.
\end{definition}

\section{Ottimizzazione vincolata}
Se cerco i punti estremanti di $f$ in un insieme non aperto non è sufficiente applicare il teorema di Fermat nella parte interna dell'insieme perché potrebbero esserci estremanti anche sul bordo.\\
STRATEGIA:
\begin{enumerate}
	\item[0] se $A$ è chiuso e limitato e $f$ è continua, allora uso Weierstrass
	\item[1] applico Fermat nell'insieme aperto $A \setminus$ bordo di $A$.
	\item[2] cerco eventuali punti estremanti di $f$ sul bordo di $A$.
\end{enumerate}
% buona notizia todo

\subsection{Vincoli di uguaglianza}
in questa sezione vogliamo determianre i massimi e imini di $f(x,y)$ quando $(x,y)$ verifica una condizione aggiuntiva della forma
\begin{equation}
	F(x,y) = 0
\end{equation}
% esempio da video todo

\begin{definition}
	Siano $A \subset \mathbb{R}^2$ aperto, $f,F \in C^2(A)$.\\
	Poniamo
	\begin{equation}
		z = {(x,y) \in A: F(x,y) = 0}
	\end{equation}
	Un punto $(x_0,y_0) \in z$ si dice:
	\begin{itemize}
		\item Punto di massimo relativo (o locale) per $f$ vincolato a $z$ se esiste $\delta > 0$ tale che
				\begin{equation}
					f(x_0,y_0) \geq f(x,y) \quad \forall (x,y) \in  B_{\delta}((x_0,y_0)) \cap z
				\end{equation}
		\item Punto di massimo assoluto per $f$ vincolato a $z$ se
				\begin{equation}
					f(x_0,y_0) \geq f(x,y) \quad \forall (x,y) \in z
				\end{equation}
		\item analogamente min
		\item punto di estremo vincolato un max vincolato o un min vincolato.
	\end{itemize}
\end{definition}

\section*{Lezione 25-11-2022}
Il metodo di sostituzione non è applicabile il vincolo non è esplicitabile.
Es: $xy = \ln y + \sqrt{x}$\\
Inoltre, ci sono casi in cui è possibile sostituire, ma bisogna prestare attenzione.\\

% integra todo importante

METODO DEI MOLTIPLICATORI DI LAGRANGE\\
Introduzione informale: Così come il teorema di Fermat fornice i candidati punti di estrempo libero, il metodo dei moltiplicatori di Lagrange fornisce i candidati punti di estremo vincolato.\\
Tra i candidati di estremo vincolato seleziono max e min assoluti vincolati confrontoando i valori di $f$.\\

\begin{thm} \textbf{Metodo dei moltiplicatori di Langrange}\\
	$A \subset \mathbb{R}^2$ aperto, $f,F \in C^1(A)$.\\
	Sia $\underline{x}_0$ un punto di estremo vincolato per $f$ sul vincolo:
	\begin{equation}
		z = {(x,y) \in A: F(x,y) = 0}
	\end{equation}
	Supponiamo inoltre che $\nabla F(\underline{x}_0) \neq 0$.\\
	Allora esiste un $\lambda_0 \in \mathbb{R}$, detto moltiplicatore di Lagrange, tale che 
	\begin{equation}
		\nabla f(\underline{x}_0) = \lambda_0 \nabla F(\underline{x}_0)
	\end{equation}
\end{thm}
\emph{Oss:} Scriviamo le condizioni:
\begin{equation}
	\nabla f(\underline{x}_0) = \lambda_0 \nabla F(\underline{x}_0) \quad \text{e} \quad (x_0,y_0) \in z
\end{equation}
In modo più esplicito:
\begin{equation}
	\begin{cases}
		\frac{\partial f}{\partial x} (x_0,y_0) = \lambda_0 \frac{\partial F}{\partial x} (x_0,y_0)\\
		\frac{\partial f}{\partial y} (x_0,y_0) = \lambda_0 \frac{\partial F}{\partial y} (x_0,y_0)\\
		F(x_0,y_0) = 0
	\end{cases}
\end{equation}
Sistema non lineare di tre equazioni nelle tre incognite $(x_0,y_0,\lambda_0)$.\\

In pratica: i punti candidati sono le soluzioni di:
\begin{equation}
	\begin{cases}
		\nabla F(\underline{x}_0) = \underline{0}\\
		F(\underline{x}_0) = 0
	\end{cases}
	\quad \text{e} \quad
	\begin{cases}
		\nabla f(\underline{x}_0) = \lambda_0 \nabla F(\underline{x}_0)\\
		F(\underline{x}_0) = 0
	\end{cases}
\end{equation}
\emph{Oss:} il valore $\lambda_0 = 0$ è ammissibile.\\
% esempio todo

% integrazione? todo

% todo importante tabella
TABELLA DA INSERIRE\\

%integra todo importante
\begin{definition}
	Si definisce Lagrangiana la funzione
	\begin{equation}
		\mathcal{L} (x,y,\lambda) = f(x,y) - \lambda F(x,y)
	\end{equation}
	Con $(x,y)\in A$ e $\lambda \in \mathbb{R}$.
\end{definition}
\emph{Oss:}\\
			$\frac{\partial \mathcal{L} }{\partial x} (x,y,\lambda) = \frac{\partial f}{\partial x} (x,y) - \lambda \frac{\partial F}{\partial x} (x,y)$\\
			$\frac{\partial \mathcal{L} }{\partial y} (x,y,\lambda) = \frac{\partial f}{\partial y} (x,y) - \lambda \frac{\partial F}{\partial y} (x,y)$\\
			$\frac{\partial \mathcal{L} }{\partial \lambda} (x,y,\lambda) = -F(x,y)$\\
Quindi il sistema dei moltiplicatori di Lagrange si può scrivere come:
\begin{equation}
	\nabla \mathcal{L} (x,y,\lambda_0) = \underline{0}
\end{equation}

\chapter{Calcolo integrale per funzioni di più variabili}
%integrali doppi
%integrali tripli
Esempi introduttivi:
\begin{itemize}
	\item Data una lamina piana $D \subseteq \mathbb{R}^2$ avente densità di massa $\rho(x,y) \geq 0 \Longrightarrow \iint_D \rho(x,y) \, dxdy$ è la massa della lamina.\\
			Momento d'inerzia rispetto all'asse $z$:
			\begin{equation}
				\iint_D \rho(x,y) (x^2 + y^2) \, dxdy
			\end{equation}
	\item Date 2 variabile aleatorie continue $X,Y$, la densità congiunta $f_{X,Y}(x,y)$ è tale che probabilità evento $A$:
		\begin{equation}
			A = P[(X,Y) \in A] = \iint_A f_{X,Y}(x,y) \, dxdy
		\end{equation}
\end{itemize}

\section{Integrali doppi}
\subsection{Regioni semplici}
\begin{definition}
	$D \subseteq \mathbb{R}^2$ è detta regione $y$-semplice se:
	\begin{equation}
		D = {(x,y) \in \mathbb{R}^2: x \in [a,b], g_1(x) \leq y \leq g_2(x)}
	\end{equation}
	Con $a,b \in \mathbb{R}$ e $g_1,g_2 \in C^1([a,b])$. % verifica importante todo
\end{definition}





\chapter{Esercizi}
\end{document}